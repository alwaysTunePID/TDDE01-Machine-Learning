NaiveBayesImpl = function(train, test) {
trainBlue = train[train$species == "Blue",]
trainOrange = train[train$species == "Orange",]
priorBlue = nrow(trainBlue)/(nrow(trainBlue) + nrow(trainOrange))
priorOrange = 1 - priorBlue
BlueCLDens = density(trainBlue$CL)
OrangeCLDens = density(trainOrange$CL)
BlueRWDens = density(trainBlue$RW)
OrangeRWDens = density(trainOrange$RW)
BlueFLDens = density(trainBlue$FL)
OrangeFLDens = density(trainOrange$FL)
BlueCWDens = density(trainBlue$CW)
OrangeCWDens = density(trainOrange$CW)
trainBlueBDDens = density(trainBlue$BD)
trainOrangeBDDens = density(trainOrange$BD)
trainFeats = as.matrix(train[,-c(1,2,3)])
testFeats = as.matrix(test[,-c(1,2,3)])
trainClassifications = factor(apply(trainFeats, 1, function(row) {
#print(row[1])
#print(row[2])
#print(row[3])
#print(row[4])
#print(row[5])
print(BlueFLDens$x)
blue = approx(BlueFLDens$x, BlueFLDens$y, xout = row[1])*approx(BlueRWDens$x, BlueRWDens$y, xout = row[2])*approx(BlueCLDens$x,BlueCLDens$y,xout=row[3])*approx(BlueCWDens$x, BlueCWDens$y, xout = row[4])*approx(BlueBDDens$x, BlueBDDens$y, xout = row[5])*priorBlue
orange = approx(OrangeFLDens$x, OrangeFLDens$y, xout = row[1])*approx(OrangeRWDens$x, OrangeRWDens$y, xout = row[2])*approx(OrangeCLDens$x,OrangeCLDens$y,xout=row[3])*approx(OrangeCWDens$x, OrangeCWDens$y, xout = row[4])*approx(OrangeBDDens$x, OrangeBDDens$y, xout = row[5])*priorOrange
return(ifelse(blue >= orange, "blue", "orange"))
}))
print(table(train$species, trainClassifications))
testClassifications = factor(apply(testFeats, 1, function(row) {
blue = approx(BlueFLDens$x, BlueFLDens$y, xout = row[1])*approx(BlueRWDens$x, BlueRWDens$y, xout = row[2])*approx(BlueCLDens$x,BlueCLDens$y,xout=row[3])*approx(BlueCWDens$x, BlueCWDens$y, xout = row[4])*approx(BlueBDDens$x, BlueBDDens$y, xout = row[5])*priorBlue
orange = approx(OrangeFLDens$x, OrangeFLDens$y, xout = row[1])*approx(OrangeRWDens$x, OrangeRWDens$y, xout = row[2])*approx(OrangeCLDens$x,OrangeCLDens$y,xout=row[3])*approx(OrangeCWDens$x, OrangeCWDens$y, xout = row[4])*approx(OrangeBDDens$x, OrangeBDDens$y, xout = row[5])*priorOrange
return(ifelse(blue >= orange, "blue", "orange"))
}))
print(table(test$species, trainClassifications))
}
NaiveBayesImpl(train, test)
# Should I use training and test? Probably yes
n=dim(data)[1]
set.seed(12345)
id=sample(1:n, floor(n*0.5))
train=data[id,]
test=data[-id,]
#plot(density(trainMale$CL))
#densMaleCL = plot(density(trainMale$CL))
#integrate(approxfun(densMaleCL), lower = train$CL[1], upper = train$CL[1])
NaiveBayesImpl = function(train, test) {
trainBlue = train[train$species == "Blue",]
trainOrange = train[train$species == "Orange",]
priorBlue = nrow(trainBlue)/(nrow(trainBlue) + nrow(trainOrange))
priorOrange = 1 - priorBlue
BlueCLDens = density(trainBlue$CL)
OrangeCLDens = density(trainOrange$CL)
BlueRWDens = density(trainBlue$RW)
OrangeRWDens = density(trainOrange$RW)
BlueFLDens = density(trainBlue$FL)
OrangeFLDens = density(trainOrange$FL)
BlueCWDens = density(trainBlue$CW)
OrangeCWDens = density(trainOrange$CW)
trainBlueBDDens = density(trainBlue$BD)
trainOrangeBDDens = density(trainOrange$BD)
trainFeats = as.matrix(train[,-c(1,2,3)])
testFeats = as.matrix(test[,-c(1,2,3)])
trainClassifications = factor(apply(trainFeats, 1, function(row) {
#print(row[1])
#print(row[2])
#print(row[3])
#print(row[4])
#print(row[5])
print(BlueFLDens$y)
blue = approx(BlueFLDens$x, BlueFLDens$y, xout = row[1])*approx(BlueRWDens$x, BlueRWDens$y, xout = row[2])*approx(BlueCLDens$x,BlueCLDens$y,xout=row[3])*approx(BlueCWDens$x, BlueCWDens$y, xout = row[4])*approx(BlueBDDens$x, BlueBDDens$y, xout = row[5])*priorBlue
orange = approx(OrangeFLDens$x, OrangeFLDens$y, xout = row[1])*approx(OrangeRWDens$x, OrangeRWDens$y, xout = row[2])*approx(OrangeCLDens$x,OrangeCLDens$y,xout=row[3])*approx(OrangeCWDens$x, OrangeCWDens$y, xout = row[4])*approx(OrangeBDDens$x, OrangeBDDens$y, xout = row[5])*priorOrange
return(ifelse(blue >= orange, "blue", "orange"))
}))
print(table(train$species, trainClassifications))
testClassifications = factor(apply(testFeats, 1, function(row) {
blue = approx(BlueFLDens$x, BlueFLDens$y, xout = row[1])*approx(BlueRWDens$x, BlueRWDens$y, xout = row[2])*approx(BlueCLDens$x,BlueCLDens$y,xout=row[3])*approx(BlueCWDens$x, BlueCWDens$y, xout = row[4])*approx(BlueBDDens$x, BlueBDDens$y, xout = row[5])*priorBlue
orange = approx(OrangeFLDens$x, OrangeFLDens$y, xout = row[1])*approx(OrangeRWDens$x, OrangeRWDens$y, xout = row[2])*approx(OrangeCLDens$x,OrangeCLDens$y,xout=row[3])*approx(OrangeCWDens$x, OrangeCWDens$y, xout = row[4])*approx(OrangeBDDens$x, OrangeBDDens$y, xout = row[5])*priorOrange
return(ifelse(blue >= orange, "blue", "orange"))
}))
print(table(test$species, trainClassifications))
}
NaiveBayesImpl(train, test)
trainBlue = train[train$species == "Blue",]
trainOrange = train[train$species == "Orange",]
priorBlue = nrow(trainBlue)/(nrow(trainBlue) + nrow(trainOrange))
priorOrange = 1 - priorBlue
BlueCLDens = density(trainBlue$CL)
OrangeCLDens = density(trainOrange$CL)
BlueRWDens = density(trainBlue$RW)
OrangeRWDens = density(trainOrange$RW)
BlueFLDens = density(trainBlue$FL)
OrangeFLDens = density(trainOrange$FL)
BlueCWDens = density(trainBlue$CW)
OrangeCWDens = density(trainOrange$CW)
trainBlueBDDens = density(trainBlue$BD)
trainOrangeBDDens = density(trainOrange$BD)
trainFeats = as.matrix(train[,-c(1,2,3)])
testFeats = as.matrix(test[,-c(1,2,3)])
approx(BlueFLDens$x, BlueFLDens$y, xout = row[1])
approx(BlueFLDens$x, BlueFLDens$y, xout = trainFeats[1,1])
n=dim(data)[1]
set.seed(12345)
id=sample(1:n, floor(n*0.5))
train=data[id,]
test=data[-id,]
#plot(density(trainMale$CL))
#densMaleCL = plot(density(trainMale$CL))
#integrate(approxfun(densMaleCL), lower = train$CL[1], upper = train$CL[1])
NaiveBayesImpl = function(train, test) {
trainBlue = train[train$species == "Blue",]
trainOrange = train[train$species == "Orange",]
priorBlue = nrow(trainBlue)/(nrow(trainBlue) + nrow(trainOrange))
priorOrange = 1 - priorBlue
BlueCLDens = density(trainBlue$CL)
OrangeCLDens = density(trainOrange$CL)
BlueRWDens = density(trainBlue$RW)
OrangeRWDens = density(trainOrange$RW)
BlueFLDens = density(trainBlue$FL)
OrangeFLDens = density(trainOrange$FL)
BlueCWDens = density(trainBlue$CW)
OrangeCWDens = density(trainOrange$CW)
trainBlueBDDens = density(trainBlue$BD)
trainOrangeBDDens = density(trainOrange$BD)
trainFeats = as.matrix(train[,-c(1,2,3)])
testFeats = as.matrix(test[,-c(1,2,3)])
trainClassifications = factor(apply(trainFeats, 1, function(row) {
#print(row[1])
#print(row[2])
#print(row[3])
#print(row[4])
#print(row[5])
#print(BlueFLDens$y)
blue = approx(BlueFLDens$x, BlueFLDens$y, xout = row[1])$y*approx(BlueRWDens$x, BlueRWDens$y, xout = row[2])$y*approx(BlueCLDens$x,BlueCLDens$y,xout=row[3])$y*approx(BlueCWDens$x, BlueCWDens$y, xout = row[4])$y*approx(BlueBDDens$x, BlueBDDens$y, xout = row[5])$y*priorBlue
orange = approx(OrangeFLDens$x, OrangeFLDens$y, xout = row[1])$y*approx(OrangeRWDens$x, OrangeRWDens$y, xout = row[2])$y*approx(OrangeCLDens$x,OrangeCLDens$y,xout=row[3])$y*approx(OrangeCWDens$x, OrangeCWDens$y, xout = row[4])$y*approx(OrangeBDDens$x, OrangeBDDens$y, xout = row[5])$y*priorOrange
return(ifelse(blue >= orange, "blue", "orange"))
}))
print(table(train$species, trainClassifications))
testClassifications = factor(apply(testFeats, 1, function(row) {
blue = approx(BlueFLDens$x, BlueFLDens$y, xout = row[1])$y*approx(BlueRWDens$x, BlueRWDens$y, xout = row[2])$y*approx(BlueCLDens$x,BlueCLDens$y,xout=row[3])$y*approx(BlueCWDens$x, BlueCWDens$y, xout = row[4])$y*approx(BlueBDDens$x, BlueBDDens$y, xout = row[5])$y*priorBlue
orange = approx(OrangeFLDens$x, OrangeFLDens$y, xout = row[1])$y*approx(OrangeRWDens$x, OrangeRWDens$y, xout = row[2])$y*approx(OrangeCLDens$x,OrangeCLDens$y,xout=row[3])$y*approx(OrangeCWDens$x, OrangeCWDens$y, xout = row[4])$y*approx(OrangeBDDens$x, OrangeBDDens$y, xout = row[5])$y*priorOrange
return(ifelse(blue >= orange, "blue", "orange"))
}))
print(table(test$species, trainClassifications))
}
NaiveBayesImpl(train, test)
n=dim(data)[1]
set.seed(12345)
id=sample(1:n, floor(n*0.5))
train=data[id,]
test=data[-id,]
#plot(density(trainMale$CL))
#densMaleCL = plot(density(trainMale$CL))
#integrate(approxfun(densMaleCL), lower = train$CL[1], upper = train$CL[1])
NaiveBayesImpl = function(train, test) {
trainBlue = train[train$species == "Blue",]
trainOrange = train[train$species == "Orange",]
priorBlue = nrow(trainBlue)/(nrow(trainBlue) + nrow(trainOrange))
priorOrange = 1 - priorBlue
BlueCLDens = density(trainBlue$CL)
OrangeCLDens = density(trainOrange$CL)
BlueRWDens = density(trainBlue$RW)
OrangeRWDens = density(trainOrange$RW)
BlueFLDens = density(trainBlue$FL)
OrangeFLDens = density(trainOrange$FL)
BlueCWDens = density(trainBlue$CW)
OrangeCWDens = density(trainOrange$CW)
BlueBDDens = density(trainBlue$BD)
OrangeBDDens = density(trainOrange$BD)
trainFeats = as.matrix(train[,-c(1,2,3)])
testFeats = as.matrix(test[,-c(1,2,3)])
trainClassifications = factor(apply(trainFeats, 1, function(row) {
#print(row[1])
#print(row[2])
#print(row[3])
#print(row[4])
#print(row[5])
#print(BlueFLDens$y)
blue = approx(BlueFLDens$x, BlueFLDens$y, xout = row[1])$y*approx(BlueRWDens$x, BlueRWDens$y, xout = row[2])$y*approx(BlueCLDens$x,BlueCLDens$y,xout=row[3])$y*approx(BlueCWDens$x, BlueCWDens$y, xout = row[4])$y*approx(BlueBDDens$x, BlueBDDens$y, xout = row[5])$y*priorBlue
orange = approx(OrangeFLDens$x, OrangeFLDens$y, xout = row[1])$y*approx(OrangeRWDens$x, OrangeRWDens$y, xout = row[2])$y*approx(OrangeCLDens$x,OrangeCLDens$y,xout=row[3])$y*approx(OrangeCWDens$x, OrangeCWDens$y, xout = row[4])$y*approx(OrangeBDDens$x, OrangeBDDens$y, xout = row[5])$y*priorOrange
return(ifelse(blue >= orange, "blue", "orange"))
}))
print(table(train$species, trainClassifications))
testClassifications = factor(apply(testFeats, 1, function(row) {
blue = approx(BlueFLDens$x, BlueFLDens$y, xout = row[1])$y*approx(BlueRWDens$x, BlueRWDens$y, xout = row[2])$y*approx(BlueCLDens$x,BlueCLDens$y,xout=row[3])$y*approx(BlueCWDens$x, BlueCWDens$y, xout = row[4])$y*approx(BlueBDDens$x, BlueBDDens$y, xout = row[5])$y*priorBlue
orange = approx(OrangeFLDens$x, OrangeFLDens$y, xout = row[1])$y*approx(OrangeRWDens$x, OrangeRWDens$y, xout = row[2])$y*approx(OrangeCLDens$x,OrangeCLDens$y,xout=row[3])$y*approx(OrangeCWDens$x, OrangeCWDens$y, xout = row[4])$y*approx(OrangeBDDens$x, OrangeBDDens$y, xout = row[5])$y*priorOrange
return(ifelse(blue >= orange, "blue", "orange"))
}))
print(table(test$species, trainClassifications))
}
NaiveBayesImpl(train, test)
## Special task 4
classificationRate = function(confMatrix) {
return(sum(diag(confMatrix))/sum(confMatrix))
}
data = read.csv("australian-crabs.csv")
n=dim(data)[1]
set.seed(12345)
id=sample(1:n, floor(n*0.5))
train=data[id,]
test=data[-id,]
#plot(density(trainMale$CL))
#densMaleCL = plot(density(trainMale$CL))
#integrate(approxfun(densMaleCL), lower = train$CL[1], upper = train$CL[1])
NaiveBayesImpl = function(train, test) {
trainBlue = train[train$species == "Blue",]
trainOrange = train[train$species == "Orange",]
priorBlue = nrow(trainBlue)/(nrow(trainBlue) + nrow(trainOrange))
priorOrange = 1 - priorBlue
BlueCLDens = density(trainBlue$CL)
OrangeCLDens = density(trainOrange$CL)
BlueRWDens = density(trainBlue$RW)
OrangeRWDens = density(trainOrange$RW)
BlueFLDens = density(trainBlue$FL)
OrangeFLDens = density(trainOrange$FL)
BlueCWDens = density(trainBlue$CW)
OrangeCWDens = density(trainOrange$CW)
BlueBDDens = density(trainBlue$BD)
OrangeBDDens = density(trainOrange$BD)
trainFeats = as.matrix(train[,-c(1,2,3)])
testFeats = as.matrix(test[,-c(1,2,3)])
trainClassifications = factor(apply(trainFeats, 1, function(row) {
blue = approx(BlueFLDens$x, BlueFLDens$y, xout = row[1])$y*approx(BlueRWDens$x, BlueRWDens$y, xout = row[2])$y*approx(BlueCLDens$x,BlueCLDens$y,xout=row[3])$y*approx(BlueCWDens$x, BlueCWDens$y, xout = row[4])$y*approx(BlueBDDens$x, BlueBDDens$y, xout = row[5])$y*priorBlue
orange = approx(OrangeFLDens$x, OrangeFLDens$y, xout = row[1])$y*approx(OrangeRWDens$x, OrangeRWDens$y, xout = row[2])$y*approx(OrangeCLDens$x,OrangeCLDens$y,xout=row[3])$y*approx(OrangeCWDens$x, OrangeCWDens$y, xout = row[4])$y*approx(OrangeBDDens$x, OrangeBDDens$y, xout = row[5])$y*priorOrange
return(ifelse(blue >= orange, "blue", "orange"))
}))
print("Confusion matrix train")
print(table(train$species, trainClassifications))
print("Misclassification rate train")
print(1 - classificationRate(table(train$species, trainClassifications)))
testClassifications = factor(apply(testFeats, 1, function(row) {
blue = approx(BlueFLDens$x, BlueFLDens$y, xout = row[1])$y*approx(BlueRWDens$x, BlueRWDens$y, xout = row[2])$y*approx(BlueCLDens$x,BlueCLDens$y,xout=row[3])$y*approx(BlueCWDens$x, BlueCWDens$y, xout = row[4])$y*approx(BlueBDDens$x, BlueBDDens$y, xout = row[5])$y*priorBlue
orange = approx(OrangeFLDens$x, OrangeFLDens$y, xout = row[1])$y*approx(OrangeRWDens$x, OrangeRWDens$y, xout = row[2])$y*approx(OrangeCLDens$x,OrangeCLDens$y,xout=row[3])$y*approx(OrangeCWDens$x, OrangeCWDens$y, xout = row[4])$y*approx(OrangeBDDens$x, OrangeBDDens$y, xout = row[5])$y*priorOrange
return(ifelse(blue >= orange, "blue", "orange"))
}))
print("Confusion matrix test")
print(table(test$species, trainClassifications))
print("Misclassification rate test")
print(1 - classificationRate(table(test$species, trainClassifications)))
}
NaiveBayesImpl(train, test)
classificationRate = function(confMatrix) {
return(sum(diag(confMatrix))/sum(confMatrix))
}
data = read.csv("australian-crabs.csv")
n=dim(data)[1]
set.seed(12345)
id=sample(1:n, floor(n*0.5))
train=data[id,]
test=data[-id,]
#plot(density(trainMale$CL))
#densMaleCL = plot(density(trainMale$CL))
#integrate(approxfun(densMaleCL), lower = train$CL[1], upper = train$CL[1])
NaiveBayesImpl = function(train, test) {
# Divide into subsets of the classes
trainBlue = train[train$species == "Blue",]
trainOrange = train[train$species == "Orange",]
# Set priors proportional to the class sizes
priorBlue = nrow(trainBlue)/(nrow(trainBlue) + nrow(trainOrange))
priorOrange = 1 - priorBlue
# Estimate the densities for the training sets through density() function for each class for each measurement variable.
BlueCLDens = density(trainBlue$CL)
OrangeCLDens = density(trainOrange$CL)
BlueRWDens = density(trainBlue$RW)
OrangeRWDens = density(trainOrange$RW)
BlueFLDens = density(trainBlue$FL)
OrangeFLDens = density(trainOrange$FL)
BlueCWDens = density(trainBlue$CW)
OrangeCWDens = density(trainOrange$CW)
BlueBDDens = density(trainBlue$BD)
OrangeBDDens = density(trainOrange$BD)
trainFeats = as.matrix(train[,-c(1,2,3)])
testFeats = as.matrix(test[,-c(1,2,3)])
# Classify
trainClassifications = factor(apply(trainFeats, 1, function(row) {
blue = approx(BlueFLDens$x, BlueFLDens$y, xout = row[1])$y*approx(BlueRWDens$x, BlueRWDens$y, xout = row[2])$y*approx(BlueCLDens$x,BlueCLDens$y,xout=row[3])$y*approx(BlueCWDens$x, BlueCWDens$y, xout = row[4])$y*approx(BlueBDDens$x, BlueBDDens$y, xout = row[5])$y*priorBlue
print(blue)
orange = approx(OrangeFLDens$x, OrangeFLDens$y, xout = row[1])$y*approx(OrangeRWDens$x, OrangeRWDens$y, xout = row[2])$y*approx(OrangeCLDens$x,OrangeCLDens$y,xout=row[3])$y*approx(OrangeCWDens$x, OrangeCWDens$y, xout = row[4])$y*approx(OrangeBDDens$x, OrangeBDDens$y, xout = row[5])$y*priorOrange
print(orange)
return(ifelse(blue >= orange, "blue", "orange"))
}))
print("Confusion matrix train")
print(table(train$species, trainClassifications))
print("Misclassification rate train")
print(1 - classificationRate(table(train$species, trainClassifications)))
testClassifications = factor(apply(testFeats, 1, function(row) {
blue = approx(BlueFLDens$x, BlueFLDens$y, xout = row[1])$y*approx(BlueRWDens$x, BlueRWDens$y, xout = row[2])$y*approx(BlueCLDens$x,BlueCLDens$y,xout=row[3])$y*approx(BlueCWDens$x, BlueCWDens$y, xout = row[4])$y*approx(BlueBDDens$x, BlueBDDens$y, xout = row[5])$y*priorBlue
orange = approx(OrangeFLDens$x, OrangeFLDens$y, xout = row[1])$y*approx(OrangeRWDens$x, OrangeRWDens$y, xout = row[2])$y*approx(OrangeCLDens$x,OrangeCLDens$y,xout=row[3])$y*approx(OrangeCWDens$x, OrangeCWDens$y, xout = row[4])$y*approx(OrangeBDDens$x, OrangeBDDens$y, xout = row[5])$y*priorOrange
return(ifelse(blue >= orange, "blue", "orange"))
}))
print("Confusion matrix test")
print(table(test$species, trainClassifications))
print("Misclassification rate test")
print(1 - classificationRate(table(test$species, trainClassifications)))
}
NaiveBayesImpl(train, test)
NaiveBayesImpl(test, train)
NaiveBayesImpl(train, train)
NaiveBayesImpl(train, test)
NaiveBayesImpl = function(train, test) {
# Divide into subsets of the classes
trainBlue = train[train$species == "Blue",]
trainOrange = train[train$species == "Orange",]
# Set priors proportional to the class sizes
priorBlue = nrow(trainBlue)/(nrow(trainBlue) + nrow(trainOrange))
priorOrange = 1 - priorBlue
# Estimate the densities for the training sets through density() function for each class for each measurement variable.
BlueCLDens = density(trainBlue$CL)
OrangeCLDens = density(trainOrange$CL)
BlueRWDens = density(trainBlue$RW)
OrangeRWDens = density(trainOrange$RW)
BlueFLDens = density(trainBlue$FL)
OrangeFLDens = density(trainOrange$FL)
BlueCWDens = density(trainBlue$CW)
OrangeCWDens = density(trainOrange$CW)
BlueBDDens = density(trainBlue$BD)
OrangeBDDens = density(trainOrange$BD)
trainFeats = as.matrix(train[,-c(1,2,3)])
testFeats = as.matrix(test[,-c(1,2,3)])
# Classify them according to unnormalized probabilities obtained through densities calculated above.
# This assumes conditional independence of the variables, just like Naive Bayes
trainClassifications = factor(apply(trainFeats, 1, function(row) {
blue = approx(BlueFLDens$x, BlueFLDens$y, xout = row[1])$y*approx(BlueRWDens$x, BlueRWDens$y, xout = row[2])$y*approx(BlueCLDens$x,BlueCLDens$y,xout=row[3])$y*approx(BlueCWDens$x, BlueCWDens$y, xout = row[4])$y*approx(BlueBDDens$x, BlueBDDens$y, xout = row[5])$y*priorBlue
orange = approx(OrangeFLDens$x, OrangeFLDens$y, xout = row[1])$y*approx(OrangeRWDens$x, OrangeRWDens$y, xout = row[2])$y*approx(OrangeCLDens$x,OrangeCLDens$y,xout=row[3])$y*approx(OrangeCWDens$x, OrangeCWDens$y, xout = row[4])$y*approx(OrangeBDDens$x, OrangeBDDens$y, xout = row[5])$y*priorOrange
return(ifelse(blue >= orange, "blue", "orange"))
}))
print("Confusion matrix train")
print(table(train$species, trainClassifications))
print("Misclassification rate train")
print(1 - classificationRate(table(train$species, trainClassifications)))
testClassifications = factor(apply(testFeats, 1, function(row) {
blue = approx(BlueFLDens$x, BlueFLDens$y, xout = row[1])$y*approx(BlueRWDens$x, BlueRWDens$y, xout = row[2])$y*approx(BlueCLDens$x,BlueCLDens$y,xout=row[3])$y*approx(BlueCWDens$x, BlueCWDens$y, xout = row[4])$y*approx(BlueBDDens$x, BlueBDDens$y, xout = row[5])$y*priorBlue
orange = approx(OrangeFLDens$x, OrangeFLDens$y, xout = row[1])$y*approx(OrangeRWDens$x, OrangeRWDens$y, xout = row[2])$y*approx(OrangeCLDens$x,OrangeCLDens$y,xout=row[3])$y*approx(OrangeCWDens$x, OrangeCWDens$y, xout = row[4])$y*approx(OrangeBDDens$x, OrangeBDDens$y, xout = row[5])$y*priorOrange
return(ifelse(blue >= orange, "blue", "orange"))
}))
print("Confusion matrix test")
print(table(test$species, trainClassifications))
print("Misclassification rate test")
print(1 - classificationRate(table(test$species, trainClassifications)))
}
NaiveBayesImpl(train, test)
plot(x=data$FL, y = data$RW)
cov(as.matrix(data[,-c(1,2,3)]))
cov(as.matrix(data[,-c(1,2,3)]))
cov(as.matrix(data[,-c(1,2,3)]))
library(e1071)
fit = naiveBayes(species ~ FW + RW + CL + CW + BD,data=train)
cov(as.matrix(data[,-c(1,2,3)]))
library(e1071)
fit = naiveBayes(species ~ FL + RW + CL + CW + BD,data=train)
trainPreds = predict(fit, train)
table(train$species, trainPreds)
testPreds = predict(fit, test)
table(test$species, testPreds)
cov(as.matrix(data[,-c(1,2,3)]))
library(e1071)
fit = naiveBayes(species ~ FL + RW + CL + CW + BD,data=train)
trainPreds = predict(fit, train)
table(train$species, trainPreds)
1 - classificationRate(table(train$species, trainPreds))
testPreds = predict(fit, test)
table(test$species, testPreds)
print("Misclassification rate test")
1 - classificationRate(table(test$species, testPreds))
library(e1071)
fit = naiveBayes(species ~ FL + RW + CL + CW + BD,data=train)
trainPreds = predict(fit, train)
table(train$species, trainPreds)
1 - classificationRate(table(train$species, trainPreds))
testPreds = predict(fit, test)
table(test$species, testPreds)
print("Misclassification rate test")
1 - classificationRate(table(test$species, testPreds))
library(MASS)
fitModel = lda(formula = sex ~ RW + CL,
data = data,
prior = c(length(data$sex[data$sex == "Male"])
/nrow(data),length(data$sex[data$sex == "Female"])
/nrow(data)))
fits = predict(fitModel, data)
confMatrix = table(data$sex, fits$class)
confMatrix
print("misclassificionation rate")
1 - classificationRate(confMatrix)
library(MASS)
fitModel = lda(formula = sex ~ RW + CL,
data = data,
prior = c(length(data$sex[data$sex == "Male"])
/nrow(data),length(data$sex[data$sex == "Female"])
/nrow(data)))
fits = predict(fitModel, train)
confMatrix = table(train$sex, fits$class)
confMatrix
print("misclassificionation rate train")
1 - classificationRate(confMatrix)
fits = predict(fitModel, test)
confMatrix = table(test$sex, fits$class)
confMatrix
print("misclassificionation rate test")
1 - classificationRate(confMatrix)
data = read.csv("australian-crabs.csv")
DiscFunc = function(x, cov_k, mu_k, prior) {
x = as.matrix(x)
mu_k = as.matrix(mu_k)
express = t(x)%*%solve(cov_k)%*%mu_k - (1/2)%*%t(mu_k)%*%solve(cov_k)%*%mu_k + log(prior)
return(express)
}
classificationRate = function(confMatrix) {
return(sum(diag(confMatrix))/sum(confMatrix))
}
# This function assumes only two dimensions of the data!
# Send in data as features as columns, data points as rows
LDA = function(x1, x2, prior1, prior2, test) {
mu_1 = apply(x1, 2, mean)
mu_2 = apply(x2, 2, mean)
cov_1 = cov(x1)
cov_2 = cov(x2)
n1 = nrow(x1)
n2 = nrow(x2)
N = n1+n2
covM = 1/N*(n1*cov_1 + n2*cov_2)
classificationsTest = apply(test, 1, function(row) {
class1 = DiscFunc(row, covM, mu_1, prior1)
class2 = DiscFunc(row, covM, mu_2, prior2)
return(which.max(c(class1,class2)))
})
classificationsTest = factor(ifelse(classificationsTest == 2, "Male", "Female"))
# Is proportional to, so we do not get the constant needed.
decisionBoundary = solve(covM)%*%(mu_2 - mu_1)
print(decisionBoundary)
return(list(classificationsTest, decisionBoundary))
}
x2 = matrix(c(data[data$sex == "Male",]$CL, data[data$sex == "Male",]$RW), nrow = nrow(data[data$sex == "Male",]), ncol = 2)
x1 = matrix(c(data[data$sex == "Female",]$CL, data[data$sex == "Female",]$RW), nrow = nrow(data[data$sex == "Female",]), ncol = 2)
test_m = matrix(c(data$CL, data$RW), nrow = nrow(data), ncol = 2)
result = LDA(x1,x2,prior1 = 0.5,prior2 = 0.5, test = test_m)
classificationsTest = result[[1]]
decisionB = result[[2]]
#print("Misclassification rate train")
#table(data$sex, classificationsTest)
#1 - classificationRate(table(data$sex, classificationsTest))
print("Misclassification rate test")
table(test$sex, classificationsTest)
length(classificationsTest)
library(MASS)
fitModel = lda(formula = sex ~ RW + CL,
data = data,
prior = c(length(data$sex[data$sex == "Male"])
/nrow(data),length(data$sex[data$sex == "Female"])
/nrow(data)))
fits = predict(fitModel, data)
confMatrix = table(data$sex, fits$class)
confMatrix
print("misclassificionation rate test")
1 - classificationRate(confMatrix)
data = read.csv("australian-crabs.csv")
DiscFunc = function(x, cov_k, mu_k, prior) {
x = as.matrix(x)
mu_k = as.matrix(mu_k)
express = t(x)%*%solve(cov_k)%*%mu_k - (1/2)%*%t(mu_k)%*%solve(cov_k)%*%mu_k + log(prior)
return(express)
}
classificationRate = function(confMatrix) {
return(sum(diag(confMatrix))/sum(confMatrix))
}
# This function assumes only two dimensions of the data!
# Send in data as features as columns, data points as rows
LDA = function(x1, x2, prior1, prior2, test) {
mu_1 = apply(x1, 2, mean)
mu_2 = apply(x2, 2, mean)
cov_1 = cov(x1)
cov_2 = cov(x2)
n1 = nrow(x1)
n2 = nrow(x2)
N = n1+n2
covM = 1/N*(n1*cov_1 + n2*cov_2)
classificationsTest = apply(test, 1, function(row) {
class1 = DiscFunc(row, covM, mu_1, prior1)
class2 = DiscFunc(row, covM, mu_2, prior2)
return(which.max(c(class1,class2)))
})
classificationsTest = factor(ifelse(classificationsTest == 2, "Male", "Female"))
# Is proportional to, so we do not get the constant needed.
decisionBoundary = solve(covM)%*%(mu_2 - mu_1)
return(list(classificationsTest, decisionBoundary))
}
x2 = matrix(c(data[data$sex == "Male",]$CL, data[data$sex == "Male",]$RW), nrow = nrow(data[data$sex == "Male",]), ncol = 2)
x1 = matrix(c(data[data$sex == "Female",]$CL, data[data$sex == "Female",]$RW), nrow = nrow(data[data$sex == "Female",]), ncol = 2)
test_m = matrix(c(data$CL, data$RW), nrow = nrow(data), ncol = 2)
result = LDA(x1,x2,prior1 = 0.5,prior2 = 0.5, test = test_m)
classificationsTest = result[[1]]
decisionB = result[[2]]
print("Misclassification rate test")
table(data$sex, classificationsTest)
1 - classificationRate(table(data$sex, classificationsTest))
library(MASS)
fitModel = lda(formula = sex ~ RW + CL,
data = data,
prior = c(length(data$sex[data$sex == "Male"])
/nrow(data),length(data$sex[data$sex == "Female"])
/nrow(data)))
fits = predict(fitModel, data)
confMatrix = table(data$sex, fits$class)
confMatrix
print("misclassificionation rate test")
1 - classificationRate(confMatrix)
plot(data[data$sex == "Male",]$CL, data[data$sex == "Male",]$RW, col = "green", xlab = "CL", ylab = "RW")
points(data[data$sex == "Female",]$CL, data[data$sex == "Female",]$RW, col = "red")
xSeq = seq(min(data$CL), max(data$CL))
c = 1.7 # How find constant c?
RWvals = -(decisionB[1,]*xSeq/decisionB[2,]) + c
lines(x=xSeq, y = RWvals)
