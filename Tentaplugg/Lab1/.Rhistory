if (i != 6) {
X_train = X_train[,-((i+1):7)]
X_test = X_test[,-((i+1):7)]
}
y_train = train$Protein
y_test = test$Protein
return(list(y_train,X_train, y_test, X_test))
#return misclassification rate and confusion matrix
}
MSE = function(y, y_hat) {
n = length(y)
return((1/n)*sum((y - y_hat)^2))
}
convert_to_poly = function(x, i) {
n = length(x)
x_matrix = matrix(c(rep(1,n), x, x^2, x^3, x^4, x^5, x^6), nrow = n, ncol = 7)
if (i != 6) {
x_matrix = x_matrix[,-((i+2):7)]
}
return(x_matrix)
}
plot(x=data$Moisture, y = data$Protein,
xlab = "Moisture",
ylab = "Protein",
main = "Comparison protein and moisture")
# Task 3
# Copying from Assignment 1
n=dim(data)[1]
set.seed(1)
id=sample(1:n, floor(n*0.5))
train=data[id,]
test=data[-id,]
MSE_vals = matrix(NA, nrow = 6, ncol = 2)
rownames(MSE_vals) = seq(1,6,1)
colnames(MSE_vals) = c("Training MSE", "Test MSE")
x_seq = seq(min(data$Moisture), max(data$Moisture), length = 1000)
colors = c("red", "blue", "green", "black", "purple", "yellow")
for (i in 1:6) {
print(i)
Model = prob_model(train, test, i)
y_train = Model[[1]]
X_train = data.frame(Model[[2]])
y_test = Model[[3]]
X_test = data.frame(Model[[4]])
if (i == 1) {
names(X_test) = c("Model..2..")
}
lm.fit = lm(y_train ~., data = X_train)
trainPredict = predict(lm.fit, X_train)
testPredict = predict(lm.fit, X_test)
betas = lm.fit$coefficients
x_matrix = convert_to_poly(x_seq, i)
lines(x=x_seq, y = t(as.matrix(betas))%*%t(x_matrix), col = colors[i], lwd = 2)
MSE_vals[i, 1] = MSE(train$Protein ,trainPredict)
MSE_vals[i, 2] = MSE(test$Protein, testPredict)
}
legend("topleft",
lty = rep(1,6),
col = colors,
lwd = c(2,2,2,2,2,2),
legend = c("Model 1",
"Model 2",
"Model 3",
"Model 4",
"Model 5",
"Model 6"))
plot(x=seq(1,6,1), y = MSE_vals[,1],
col = "green",
type = "l",
xlab = "Model",
ylab = "MSE",
lwd = 3,
main = "Training MSE", ylim = c(2.3, max(MSE_vals)))
lines(x=seq(1,6,1), y = MSE_vals[,2],
col = "blue",
lwd = 3,
xlab = "Model",
ylab = "MSE",
main = "Test MSE")
legend("bottomleft",
lty = rep(1,6),
col = c("green", "blue"),
lwd = c(3,3),
legend = c("Training MSE",
"Test MSE"))
n=dim(data)[1]
set.seed(12345)
id=sample(1:n, floor(n*0.5))
train=data[id,]
test=data[-id,]
X_train = train$Moisture
y_train = train$Protein
X_test = test$Moisture
y_test = test$Protein
errors = matrix(NA, nrow=6, ncol = 2)
colnames(errors) = c("training MSE", "test MSE")
for (i in 1:6) {
errors[i,] = polynomial_model(X_train = X_train,y_train = y_train, X_test = X_test, y_test = y_test, degree=i)
}
plot(x=seq(1,nrow(errors),1),y=errors[,1], type = "l", lwd = 3, col = "blue", ylim=c(2.5,max(errors)))
lines(x=seq(1,6,1), y=errors[,2], col="red", lwd=3)
which.min(errors[,2])
x_train = matrix(NA, nrow=nrow(as.matrix(X_train)), ncol = degree)
for (i in 1:degree) {
x_train[,i] = X_train^i
}
x_test = matrix(NA, nrow = nrow(as.matrix(X_test)), ncol = degree)
for (i in 1:degree) {
x_test[,i] = X_test^i
}
y = y_train
train = data.frame(x_train,y)
y = y_test
if (degree == 1) {
x_train = x_test
test = data.frame(x_train, y)
} else {
test = data.frame(x_test, y)
}
lm_model = lm(y~., data = train)
preds_train = predict(lm_model, train)
preds_test = predict(lm_model, test)
err_train = MSE(y=y_train, y_hat = preds_train)
err_test = MSE(y=y_test, y_hat=preds_test)
return(c(err_train, err_test))
x_train = matrix(NA, nrow=nrow(as.matrix(X_train)), ncol = degree)
for (i in 1:degree) {
x_train[,i] = X_train^i
}
n=dim(data)[1]
set.seed(12345)
id=sample(1:n, floor(n*0.5))
train=data[id,]
test=data[-id,]
train$Moisture
library(readxl)
data = read_excel("tecator.xlsx")
plot(x=data$Moisture, y = data$Protein)
# It looks like it might be described by a linear model, yes.
MSE = function(y, y_hat) {
n = length(y)
return((1/n)*sum((y - y_hat)^2))
}
polynomial_model = function(X_train, y_train, X_test, y_test, degree) {
x_train = matrix(NA, nrow=nrow(as.matrix(X_train)), ncol = degree)
for (i in 1:degree) {
x_train[,i] = X_train^i
}
x_test = matrix(NA, nrow = nrow(as.matrix(X_test)), ncol = degree)
for (i in 1:degree) {
x_test[,i] = X_test^i
}
y = y_train
train = data.frame(x_train,y)
y = y_test
if (degree == 1) {
x_train = x_test
test = data.frame(x_train, y)
} else {
test = data.frame(x_test, y)
}
lm_model = lm(y~., data = train)
preds_train = predict(lm_model, train)
preds_test = predict(lm_model, test)
err_train = MSE(y=y_train, y_hat = preds_train)
err_test = MSE(y=y_test, y_hat=preds_test)
return(c(err_train, err_test))
}
n=dim(data)[1]
set.seed(12345)
id=sample(1:n, floor(n*0.5))
train=data[id,]
test=data[-id,]
X_train = train$Protein
y_train = train$Moisture
X_test = test$Protein
y_test = test$Moisture
errors = matrix(NA, nrow=6, ncol = 2)
colnames(errors) = c("training MSE", "test MSE")
for (i in 1:6) {
errors[i,] = polynomial_model(X_train = X_train,y_train = y_train, X_test = X_test, y_test = y_test, degree=i)
}
plot(x=seq(1,nrow(errors),1),y=errors[,1], type = "l", lwd = 3, col = "blue", ylim=c(2.5,max(errors)))
lines(x=seq(1,6,1), y=errors[,2], col="red", lwd=3)
which.min(errors[,2])
library(readxl)
data = read_excel("tecator.xlsx")
plot(x=data$Moisture, y = data$Protein)
# It looks like it might be described by a linear model, yes.
MSE = function(y, y_hat) {
n = length(y)
return((1/n)*sum((y - y_hat)^2))
}
polynomial_model = function(X_train, y_train, X_test, y_test, degree) {
x_train = matrix(NA, nrow=nrow(as.matrix(X_train)), ncol = degree)
for (i in 1:degree) {
x_train[,i] = X_train^i
}
x_test = matrix(NA, nrow = nrow(as.matrix(X_test)), ncol = degree)
for (i in 1:degree) {
x_test[,i] = X_test^i
}
y = y_train
train = data.frame(x_train,y)
y = y_test
if (degree == 1) {
x_train = x_test
test = data.frame(x_train, y)
} else {
test = data.frame(x_test, y)
}
lm_model = lm(y~., data = train)
preds_train = predict(lm_model, train)
preds_test = predict(lm_model, test)
err_train = MSE(y=y_train, y_hat = preds_train)
err_test = MSE(y=y_test, y_hat=preds_test)
return(c(err_train, err_test))
}
n=dim(data)[1]
set.seed(12345)
id=sample(1:n, floor(n*0.5))
train=data[id,]
test=data[-id,]
X_train = train$Protein
y_train = train$Moisture
X_test = test$Protein
y_test = test$Moisture
errors = matrix(NA, nrow=6, ncol = 2)
colnames(errors) = c("training MSE", "test MSE")
for (i in 1:6) {
errors[i,] = polynomial_model(X_train = X_train,y_train = y_train, X_test = X_test, y_test = y_test, degree=i)
}
plot(x=seq(1,nrow(errors),1),y=errors[,1], type = "l", lwd = 3, col = "blue", ylim=c(30,max(errors)))
lines(x=seq(1,6,1), y=errors[,2], col="red", lwd=3)
which.min(errors[,2])
plot(x=data$Protein, y = data$Moisture,
xlab = "Protein",
ylab = "Moisture",
main = "Comparison protein and moisture")
# Task 3
# Copying from Assignment 1
n=dim(data)[1]
set.seed(1)
id=sample(1:n, floor(n*0.5))
train=data[id,]
test=data[-id,]
MSE_vals = matrix(NA, nrow = 6, ncol = 2)
rownames(MSE_vals) = seq(1,6,1)
colnames(MSE_vals) = c("Training MSE", "Test MSE")
x_seq = seq(min(data$Moisture), max(data$Moisture), length = 1000)
colors = c("red", "blue", "green", "black", "purple", "yellow")
for (i in 1:6) {
print(i)
Model = prob_model(train, test, i)
y_train = Model[[1]]
X_train = data.frame(Model[[2]])
y_test = Model[[3]]
X_test = data.frame(Model[[4]])
if (i == 1) {
names(X_test) = c("Model..2..")
}
lm.fit = lm(y_train ~., data = X_train)
trainPredict = predict(lm.fit, X_train)
testPredict = predict(lm.fit, X_test)
betas = lm.fit$coefficients
x_matrix = convert_to_poly(x_seq, i)
lines(x=x_seq, y = t(as.matrix(betas))%*%t(x_matrix), col = colors[i], lwd = 2)
MSE_vals[i, 1] = MSE(train$Protein ,trainPredict)
MSE_vals[i, 2] = MSE(test$Protein, testPredict)
}
legend("topleft",
lty = rep(1,6),
col = colors,
lwd = c(2,2,2,2,2,2),
legend = c("Model 1",
"Model 2",
"Model 3",
"Model 4",
"Model 5",
"Model 6"))
plot(x=seq(1,6,1), y = MSE_vals[,1],
col = "green",
type = "l",
xlab = "Model",
ylab = "MSE",
lwd = 3,
main = "Training MSE", ylim = c(2.3, max(MSE_vals)))
lines(x=seq(1,6,1), y = MSE_vals[,2],
col = "blue",
lwd = 3,
xlab = "Model",
ylab = "MSE",
main = "Test MSE")
legend("bottomleft",
lty = rep(1,6),
col = c("green", "blue"),
lwd = c(3,3),
legend = c("Training MSE",
"Test MSE"))
plot(x=data$Protein, y = data$Moisture,
xlab = "Protein",
ylab = "Moisture",
main = "Comparison protein and moisture")
# Task 3
# Copying from Assignment 1
n=dim(data)[1]
set.seed(1)
id=sample(1:n, floor(n*0.5))
train=data[id,]
test=data[-id,]
MSE_vals = matrix(NA, nrow = 6, ncol = 2)
rownames(MSE_vals) = seq(1,6,1)
colnames(MSE_vals) = c("Training MSE", "Test MSE")
x_seq = seq(min(data$Moisture), max(data$Moisture), length = 1000)
colors = c("red", "blue", "green", "black", "purple", "yellow")
for (i in 1:6) {
print(i)
Model = prob_model(train, test, i)
y_train = Model[[1]]
X_train = data.frame(Model[[2]])
y_test = Model[[3]]
X_test = data.frame(Model[[4]])
if (i == 1) {
names(X_test) = c("Model..2..")
}
lm.fit = lm(y_train ~., data = X_train)
trainPredict = predict(lm.fit, X_train)
testPredict = predict(lm.fit, X_test)
betas = lm.fit$coefficients
x_matrix = convert_to_poly(x_seq, i)
lines(x=x_seq, y = t(as.matrix(betas))%*%t(x_matrix), col = colors[i], lwd = 2)
MSE_vals[i, 1] = MSE(train$Protein ,trainPredict)
MSE_vals[i, 2] = MSE(test$Protein, testPredict)
}
legend("topleft",
lty = rep(1,6),
col = colors,
lwd = c(2,2,2,2,2,2),
legend = c("Model 1",
"Model 2",
"Model 3",
"Model 4",
"Model 5",
"Model 6"))
plot(x=seq(1,6,1), y = MSE_vals[,1],
col = "green",
type = "l",
xlab = "Model",
ylab = "MSE",
lwd = 3,
main = "Training MSE", ylim = c(2.3, max(MSE_vals)))
lines(x=seq(1,6,1), y = MSE_vals[,2],
col = "blue",
lwd = 3,
xlab = "Model",
ylab = "MSE",
main = "Test MSE")
legend("bottomleft",
lty = rep(1,6),
col = c("green", "blue"),
lwd = c(3,3),
legend = c("Training MSE",
"Test MSE"))
plot(x=data$Protein, y = data$Moisture,
xlab = "Protein",
ylab = "Moisture",
main = "Comparison protein and moisture")
library(readxl)
data = read_excel("tecator.xlsx")
plot(x=data$Moisture, y = data$Protein)
# It looks like it might be described by a linear model, yes.
MSE = function(y, y_hat) {
n = length(y)
return((1/n)*sum((y - y_hat)^2))
}
polynomial_model = function(X_train, y_train, X_test, y_test, degree) {
x_train = matrix(NA, nrow=nrow(as.matrix(X_train)), ncol = degree)
for (i in 1:degree) {
x_train[,i] = X_train^i
}
x_test = matrix(NA, nrow = nrow(as.matrix(X_test)), ncol = degree)
for (i in 1:degree) {
x_test[,i] = X_test^i
}
y = y_train
train = data.frame(x_train,y)
y = y_test
if (degree == 1) {
x_train = x_test
test = data.frame(x_train, y)
} else {
test = data.frame(x_test, y)
}
lm_model = lm(y~., data = train)
preds_train = predict(lm_model, train)
preds_test = predict(lm_model, test)
err_train = MSE(y=y_train, y_hat = preds_train)
err_test = MSE(y=y_test, y_hat=preds_test)
return(c(err_train, err_test))
}
n=dim(data)[1]
set.seed(12345)
id=sample(1:n, floor(n*0.5))
train=data[id,]
test=data[-id,]
X_train = train$Protein
y_train = train$Moisture
X_test = test$Protein
y_test = test$Moisture
errors = matrix(NA, nrow=6, ncol = 2)
colnames(errors) = c("training MSE", "test MSE")
for (i in 1:6) {
errors[i,] = polynomial_model(X_train = X_train,y_train = y_train, X_test = X_test, y_test = y_test, degree=i)
}
plot(x=seq(1,nrow(errors),1),y=errors[,1], type = "l", lwd = 3, col = "blue", ylim=c(30,max(errors)))
lines(x=seq(1,6,1), y=errors[,2], col="red", lwd=3)
which.min(errors[,2])
help(stepAIC)
??stepAIC
library(MASS)
to_remove = c("Sample","Moisture", "Protein")
!(colnames(data) %in% to_remove)
data_task_4 = data[,!(colnames(data) %in% to_remove)]
lm_model_task_4 = lm(Fat~., data = data_task_4)
###### Assignment 4 #####
library(readxl)
data = read_excel("tecator.xlsx")
library(MASS)
to_remove = c("Sample","Moisture", "Protein")
data_task_4 = data[,!(colnames(data) %in% to_remove)]
lm_model_task_4 = lm(Fat~., data = data_task_4)
reduction = stepAIC(lm_model_task_4)
reduction$coefficients
summary(reduction)
library(glmnet)
help(glmnet)
as.matrix(data_task_4[,-101])
dim(as.matrix(data_task_4[,-101]))
X_ridge = as.matrix(data_task_4[,-101])
y_ridge = as.matrix(data_task_4[,101])
ridge_reg_model = glmnet(x = X_ridge, y = y_ridge, family = "gaussian", alpha = 0)
plot(ridge_reg_model)
X_ridge = apply(as.matrix(data_task_4[,-101]),2,scale)
y_ridge = scale(as.matrix(data_task_4[,101]))
ridge_reg_model = glmnet(x = X_ridge, y = y_ridge, family = "gaussian", alpha = 0)
plot(ridge_reg_model)
ridge_reg_model = glmnet(x = X_ridge, y = y_ridge, alpha = 0)
plot(ridge_reg_model)
variablesSelected = names(reduction$coefficients)
variablesSelected = variablesSelected[-c(1)]
dataSteps = data_to_use[,variablesSelected]
dataSteps = data_task_4[,variablesSelected]
dataSteps = scale(dataSteps)
response = scale(dataToUse$Fat)
ridgeRegModel = glmnet(as.matrix(dataSteps), response, alpha = 0, family="gaussian")
plot(ridgeRegModel, xvar="lambda", label=TRUE)
response = scale(data_task_4$Fat)
ridgeRegModel = glmnet(as.matrix(dataSteps), response, alpha = 0, family="gaussian")
plot(ridgeRegModel, xvar="lambda", label=TRUE)
dataSteps = data_task_4
dataSteps = scale(dataSteps)
response = scale(data_task_4$Fat)
ridgeRegModel = glmnet(as.matrix(dataSteps), response, alpha = 0, family="gaussian")
plot(ridgeRegModel, xvar="lambda", label=TRUE)
dataSteps = data_task_4[,-c(101)]
dataSteps = scale(dataSteps)
response = scale(data_task_4$Fat)
ridgeRegModel = glmnet(as.matrix(dataSteps), response, alpha = 0, family="gaussian")
plot(ridgeRegModel, xvar="lambda", label=TRUE)
X_ridge = scale(as.matrix(data_task_4))
y_ridge = scale(as.matrix(data_task_4[,101]))
ridge_reg_model = glmnet(x = X_ridge, y = y_ridge, alpha = 0, family = "gaussian")
plot(ridge_reg_model)
X_ridge = scale(data_task_4[,-101])
y_ridge = scale(data_task_4[,101])
ridge_reg_model = glmnet(x = X_ridge, y = y_ridge, alpha = 0, family = "gaussian")
plot(ridge_reg_model)
ridge_reg_model = glmnet(X_ridge, y_ridge, alpha = 0, family = "gaussian")
plot(ridge_reg_model)
# Task 5 - ridge regression
library(glmnet)
variablesSelected = names(reduction$coefficients)
variablesSelected = variablesSelected[-c(1)]
dataSteps = data_task_4[,-c(101)]
dataSteps = scale(dataSteps)
response = scale(data_task_4$Fat)
ridgeRegModel = glmnet(as.matrix(dataSteps), response, alpha = 0, family="gaussian")
plot(ridgeRegModel, xvar="lambda", label=TRUE)
X_ridge = scale(data_task_4[,-101])
y_ridge = scale(data_task_4[,101])
ridge_reg_model = glmnet(X_ridge, y_ridge, alpha = 0, family = "gaussian")
plot(ridge_reg_model, xvar="lambda", label=T)
X_ridge = scale(data_task_4[,-101])
y_ridge = scale(data_task_4[,101])
ridge_reg_model = glmnet(X_ridge, y_ridge, alpha = 0, family = "gaussian")
plot(ridge_reg_model, xvar="lambda", label=T)
X_lasso = X_ridge
y_lasso = y_ridge
lasso_model = glmnet(X_ridge, y_ridge, alpha = 1, family = "gaussian")
plot(ridge_reg_model, xvar="lambda", label=T)
plot(lasso_model, xvar="lambda", label=T)
help(cv.glmnet)
seq(0,5,length=1000)
seq(0,5,length=100)
cv_lasso = cv.glmnet(X_lasso, y_lasso, lambda = seq(0,5,length=100), alpha = 1)
cv_lasso$lambda.min
plot(cv_lasso)
coef(cv_lasso, s = "lambda.min")
cv_lasso = cv.glmnet(X_lasso, y_lasso, lambda = seq(0,5,length=1000), alpha = 1)
cv_lasso$lambda.min
plot(cv_lasso)
coef(cv_lasso, s = "lambda.min")
