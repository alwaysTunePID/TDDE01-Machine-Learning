x_train = model.matrix(~ .-1, train[,-16])
y_train = model.matrix(~ .-1, train[,16])
y_train = train[,16]
set.seed(12345)
model = glmnet(x=x_train, y=y_train, family="binomial", alpha = 1)
plot(model, xvar="lambda", label=TRUE, main ="Cross-Validation LASSO")
lasso_cv = cv.glmnet(x=x_train, y=y_train, family = "binomial", alpha = 1)
plot(lasso_cv, xvar="lambda", label = TRUE)
warnings()
lambda_min = lasso_cv$lambda.min
E = function(y, p) {
return(sum(y*log(p) + (1-y)*log(1-p)))
}
x_test = model.matrix(~ .-1, test[,-16])
y_test = test[,16]
preds_lasso = predict(model, x_test, type = "response")
preds_tree = predict(pruned_tree, test)
E_lasso = E(as.numeric(test$Class), preds_lasso)
E_tree = E(as.numeric(test$Class), preds_tree[,2])
E_lasso
E_tree
preds_lasso = predict(model, x_test, type = "vector")
preds_tree = predict(pruned_tree, test, type = "vector")
preds_lasso = predict(model, x_test)
preds_tree = predict(pruned_tree, test, type = "vector")
preds_lasso
preds_tree
preds_tree = predict(pruned_tree, test, type = "vector")[,1]
E_lasso = E(as.numeric(test$Class), preds_lasso)
E_tree = E(as.numeric(test$Class), preds_tree[,2])
E_lasso
E_tree
preds_lasso = predict(model, x_test, s = "lambda.min", type = "response")
preds_lasso = predict(lasso_cv, x_test, s = "lambda.min", type = "response")
preds_tree = predict(pruned_tree, test, type = "vector")[,1]
E_lasso = E(as.numeric(test$Class), preds_lasso)
E_lasso
E_tree = E(as.numeric(test$Class), preds_tree[,2])
E_tree = E(as.numeric(test$Class), preds_tree)
E_tree
library(mboost)
bf = read.csv2("bodyfatregression.csv")
set.seed(1234567890)
m = blackboost(Bodyfat_percent ~ Waist_cm + Weight_kg, data = bf)
mstop(m)
cvf = cv(model.weights(m))
cvm = cvrisk(m, folds=cvf, grid=1:100)
plot(cvm)
mstop(cvm)
library(kernlab)
# Splits the data into K folds. Default set to 5.
# X is a matrix containing features
# Y is a vector (or Nx1 matrix) containing response variables
split_into_folds = function(X, Y, K = 5, seed = 12345) {
smp_size = floor(nrow(X)/K)
indices_chosen = as.numeric()
folds = list()
set.seed(seed)
for (k in 1:(K-1)) {
indices = sample(x=seq(1,nrow(X),1)[!(seq(1,nrow(X),1) %in% indices_chosen)], size=smp_size, replace = FALSE)
x = X[indices,]
y = Y[indices]
fold = as.matrix(data.frame(x,y))
folds[[k]] = fold
indices_chosen = append(indices_chosen, indices)
}
# last fold
indices = seq(1,nrow(X),1)[!seq(1,nrow(X),1) %in% indices_chosen]
x = X[indices,]
y = Y[indices]
fold = as.matrix(data.frame(x,y))
folds[[K]] = fold
return(folds)
}
# Divides folds generated by split_into_folds into a training and validation set
CVtrainAndValidationSet <- function(folds, splitIndex = -1) {
if (splitIndex == -1 || splitIndex > length(folds)) {
testIndex = floor(runif(1, min = 1, max = length(folds)))
test = folds[[testIndex]]
train = folds
train[[testIndex]] = NULL
} else {
test = folds[[splitIndex]]
train = folds
train[[splitIndex]] = NULL
}
trainSet = train[[1]]
for (i in 2:length(train)) {
trainSet = rbind(trainSet, train[[i]])
}
return(list(trainSet,test))
}
data(spam)
folds = split_into_folds(X=as.matrix(spam[,1:57]), y = spam$type, K = 2)
# Splits the data into K folds. Default set to 5.
# X is a matrix containing features
# Y is a vector (or Nx1 matrix) containing response variables
split_into_folds = function(X, Y, K = 5, seed = 12345) {
smp_size = floor(nrow(X)/K)
indices_chosen = as.numeric()
folds = list()
set.seed(seed)
for (k in 1:(K-1)) {
indices = sample(x=seq(1,nrow(X),1)[!(seq(1,nrow(X),1) %in% indices_chosen)], size=smp_size, replace = FALSE)
x = X[indices,]
y = Y[indices]
fold = as.matrix(data.frame(x,y))
folds[[k]] = fold
indices_chosen = append(indices_chosen, indices)
}
# last fold
indices = seq(1,nrow(X),1)[!seq(1,nrow(X),1) %in% indices_chosen]
x = X[indices,]
y = Y[indices]
fold = as.matrix(data.frame(x,y))
folds[[K]] = fold
return(folds)
}
# Divides folds generated by split_into_folds into a training and validation set
CVtrainAndValidationSet <- function(folds, splitIndex = -1) {
if (splitIndex == -1 || splitIndex > length(folds)) {
testIndex = floor(runif(1, min = 1, max = length(folds)))
test = folds[[testIndex]]
train = folds
train[[testIndex]] = NULL
} else {
test = folds[[splitIndex]]
train = folds
train[[splitIndex]] = NULL
}
trainSet = train[[1]]
for (i in 2:length(train)) {
trainSet = rbind(trainSet, train[[i]])
}
return(list(trainSet,test))
}
data(spam)
folds = split_into_folds(X=as.matrix(spam[,1:57]), y = spam$type, K = 2)
folds = split_into_folds(X=as.matrix(spam[,1:57]), Y = spam$type, K = 2)
folds[[1]]
# Splits the data into K folds. Default set to 5.
# X is a matrix containing features
# Y is a vector (or Nx1 matrix) containing response variables
split_into_folds = function(X, Y, K = 5, seed = 12345, response_name) {
smp_size = floor(nrow(X)/K)
indices_chosen = as.numeric()
folds = list()
set.seed(seed)
for (k in 1:(K-1)) {
indices = sample(x=seq(1,nrow(X),1)[!(seq(1,nrow(X),1) %in% indices_chosen)], size=smp_size, replace = FALSE)
x = X[indices,]
y = Y[indices]
fold = as.matrix(data.frame(x,type = y))
folds[[k]] = fold
indices_chosen = append(indices_chosen, indices)
}
# last fold
indices = seq(1,nrow(X),1)[!seq(1,nrow(X),1) %in% indices_chosen]
x = X[indices,]
y = Y[indices]
fold = as.matrix(data.frame(x,y))
folds[[K]] = fold
return(folds)
}
folds = split_into_folds(X=as.matrix(spam[,1:57]), Y = spam$type, K = 2, response_name = "type")
head(folds[[2]])
# Splits the data into K folds. Default set to 5.
# X is a matrix containing features
# Y is a vector (or Nx1 matrix) containing response variables
split_into_folds = function(X, Y, K = 5, seed = 12345, response_name) {
smp_size = floor(nrow(X)/K)
indices_chosen = as.numeric()
folds = list()
set.seed(seed)
for (k in 1:(K-1)) {
indices = sample(x=seq(1,nrow(X),1)[!(seq(1,nrow(X),1) %in% indices_chosen)], size=smp_size, replace = FALSE)
x = X[indices,]
y = Y[indices]
fold = as.matrix(data.frame(x,type = y))
folds[[k]] = fold
indices_chosen = append(indices_chosen, indices)
}
# last fold
indices = seq(1,nrow(X),1)[!seq(1,nrow(X),1) %in% indices_chosen]
x = X[indices,]
y = Y[indices]
fold = as.matrix(data.frame(x,type = y))
folds[[K]] = fold
return(folds)
}
folds = split_into_folds(X=as.matrix(spam[,1:57]), Y = spam$type, K = 2, response_name = "type")
head(folds[[2]])
5 % 3
5 mod 3
mod(5,3)
??mod
5 %% 3
2 %% 1
Cs = c(1,5)
widths = c(0.01,0.05)
help(ksvm)
results = matrix(NA, ncol=3, nrow = 6)
colnames(results) = c("C", "Width", "Error")
i = 1
for (C in Cs) {
for(width in widths) {
err = as.numeric()
for(j in 1:2) {
model = ksvm(type ~., data = folds[[i]], C = C, cross = 2, kernel = "rbfdot", kpar=list(sigma=width))
test = folds[[i %% 2 + 1]]
table(test$type, predict(model, test))
err[j] = 1 - sum(diag(table(test$type, predict(model, test))))/sum(table(test$type, predict(model, test)))
}
results[i,] = c(C, width, mean(err))
i = i + 1
}
}
for (C in Cs) {
err = as.numeric()
for(j in 1:2) {
model = ksvm(type ~., data = folds[[i]], C = C, cross = 2, kernel = "vanilladot")
test = folds[[i %% 2 + 1]]
table(test$type, predict(model, test))
err[j] = 1 - sum(diag(table(test$type, predict(model, test))))/sum(table(test$type, predict(model, test)))
}
print(mean(err))
}
for (C in Cs) {
for(width in widths) {
err = as.numeric()
for(j in 1:2) {
model = ksvm(type ~., data = folds[[i]], C = C, cross = 2, kernel = "rbfdot", kpar=list(sigma=width))
test = folds[[i %% 2 + 1]]
print("here")
print(test)
table(test$type, predict(model, test))
print("here too")
err[j] = 1 - sum(diag(table(test$type, predict(model, test))))/sum(table(test$type, predict(model, test)))
print(" here as well")
}
results[i,] = c(C, width, mean(err))
i = i + 1
}
}
typeof(folds[[1]])
folds[[1]]
folds[[1]]$test
data.frame(folds[[1]])
a = data.frame(folds[[1]])
library(kernlab)
# Splits the data into K folds. Default set to 5.
# X is a matrix containing features
# Y is a vector (or Nx1 matrix) containing response variables
split_into_folds = function(X, Y, K = 5, seed = 12345, response_name) {
smp_size = floor(nrow(X)/K)
indices_chosen = as.numeric()
folds = list()
set.seed(seed)
for (k in 1:(K-1)) {
indices = sample(x=seq(1,nrow(X),1)[!(seq(1,nrow(X),1) %in% indices_chosen)], size=smp_size, replace = FALSE)
x = X[indices,]
y = Y[indices]
fold = as.matrix(data.frame(as.numeric(x),type = as.factor(y)))
folds[[k]] = fold
indices_chosen = append(indices_chosen, indices)
}
# last fold
indices = seq(1,nrow(X),1)[!seq(1,nrow(X),1) %in% indices_chosen]
x = X[indices,]
y = Y[indices]
fold = as.matrix(data.frame(as.numeric(x),type = as.factor(y)))
folds[[K]] = fold
return(folds)
}
# Divides folds generated by split_into_folds into a training and validation set
CVtrainAndValidationSet <- function(folds, splitIndex = -1) {
if (splitIndex == -1 || splitIndex > length(folds)) {
testIndex = floor(runif(1, min = 1, max = length(folds)))
test = folds[[testIndex]]
train = folds
train[[testIndex]] = NULL
} else {
test = folds[[splitIndex]]
train = folds
train[[splitIndex]] = NULL
}
trainSet = train[[1]]
for (i in 2:length(train)) {
trainSet = rbind(trainSet, train[[i]])
}
return(list(trainSet,test))
}
data(spam)
folds = split_into_folds(X=as.matrix(spam[,1:57]), Y = spam$type, K = 2, response_name = "type")
folds[[1]]
as.numeric(folds[[1]][,1:57]),as.factor(folds[[1]][,58])
a = data.frame(as.numeric(folds[[1]][,1:57]),as.factor(folds[[1]][,58]))
folds[[1]][,1]
# Splits the data into K folds. Default set to 5.
# X is a matrix containing features
# Y is a vector (or Nx1 matrix) containing response variables
crossValidationSplits <- function(dataSet, k, seed = 123) {
smp_size = floor(nrow(dataSet)/k)
## set the seed to make your partition reproducible
set.seed(seed)
folds <- list()
for (i in 1:k) {
newFold <- sample(seq_len(nrow(dataSet)), size = smp_size)
folds[[i]] = data.frame(dataSet[newFold,])
dataSet <- dataSet[-newFold,]
}
return(folds)
}
# Divides folds generated by split_into_folds into a training and validation set
CVtrainAndValidationSet <- function(folds, splitIndex = -1) {
if (splitIndex == -1 || splitIndex > length(folds)) {
testIndex = floor(runif(1, min = 1, max = length(folds)))
test = folds[[testIndex]]
train = folds
train[[testIndex]] = NULL
} else {
test = folds[[splitIndex]]
train = folds
train[[splitIndex]] = NULL
}
trainSet = train[[1]]
for (i in 2:length(train)) {
trainSet = rbind(trainSet, train[[i]])
}
return(list(trainSet,test))
}
data(spam)
folds = crossValidationSplits(spam, K = 2, seed = 12345)
folds = crossValidationSplits(spam, k = 2, seed = 12345)
Cs = c(1,5)
widths = c(0.01,0.05)
results = matrix(NA, ncol=3, nrow = 6)
colnames(results) = c("C", "Width", "Error")
i = 1
for (C in Cs) {
for(width in widths) {
err = as.numeric()
for(j in 1:2) {
model = ksvm(type ~., data = folds[[i]], C = C, cross = 2, kernel = "rbfdot", kpar=list(sigma=width))
test = folds[[i %% 2 + 1]]
print("here")
print(test)
table(test$type, predict(model, test))
print("here too")
err[j] = 1 - sum(diag(table(test$type, predict(model, test))))/sum(table(test$type, predict(model, test)))
print(" here as well")
}
results[i,] = c(C, width, mean(err))
i = i + 1
}
}
for (C in Cs) {
for(width in widths) {
err = as.numeric()
for(j in 1:2) {
model = ksvm(type ~., data = folds[[j]], C = C, cross = 2, kernel = "rbfdot", kpar=list(sigma=width))
test = folds[[i %% 2 + 1]]
print("here")
print(test)
table(test$type, predict(model, test))
print("here too")
err[j] = 1 - sum(diag(table(test$type, predict(model, test))))/sum(table(test$type, predict(model, test)))
print(" here as well")
}
results[i,] = c(C, width, mean(err))
i = i + 1
}
}
results = matrix(NA, ncol=3, nrow = 6)
colnames(results) = c("C", "Width", "Error")
i = 1
for (C in Cs) {
for(width in widths) {
err = as.numeric()
for(j in 1:2) {
model = ksvm(type ~., data = folds[[j]], C = C, cross = 2, kernel = "rbfdot", kpar=list(sigma=width))
test = folds[[i %% 2 + 1]]
print("here")
print(test)
table(test$type, predict(model, test))
print("here too")
err[j] = 1 - sum(diag(table(test$type, predict(model, test))))/sum(table(test$type, predict(model, test)))
print(" here as well")
}
results[i,] = c(C, width, mean(err))
i = i + 1
}
}
data(spam)
folds = crossValidationSplits(spam, k = 2, seed = 12345)
Cs = c(1,5)
widths = c(0.01,0.05)
results = matrix(NA, ncol=3, nrow = 6)
colnames(results) = c("C", "Width", "Error")
i = 1
for (C in Cs) {
for(width in widths) {
err = as.numeric()
for(j in 1:2) {
model = ksvm(type ~., data = folds[[j]], C = C, cross = 2, kernel = "rbfdot", kpar=list(sigma=width))
test = folds[[i %% 2 + 1]]
#print("here")
#print(test)
table(test$type, predict(model, test))
#print("here too")
err[j] = 1 - sum(diag(table(test$type, predict(model, test))))/sum(table(test$type, predict(model, test)))
#print(" here as well")
}
results[i,] = c(C, width, mean(err))
i = i + 1
}
}
crossValidationSplits <- function(dataSet, k, seed = 123) {
smp_size = floor(nrow(dataSet)/k)
## set the seed to make your partition reproducible
set.seed(seed)
folds <- list()
for (i in 1:k) {
newFold <- sample(seq_len(nrow(dataSet)), size = smp_size)
folds[[i]] = data.frame(dataSet[newFold,])
dataSet <- dataSet[-newFold,]
}
return(folds)
}
# Splits the data into K folds. Default set to 5.
# X is a matrix containing features
# Y is a vector (or Nx1 matrix) containing response variables
crossValidationSplits <- function(dataSet, k, seed = 123) {
smp_size = floor(nrow(dataSet)/k)
## set the seed to make your partition reproducible
set.seed(seed)
folds <- list()
for (i in 1:k) {
newFold <- sample(seq_len(nrow(dataSet)), size = smp_size)
folds[[i]] = data.frame(dataSet[newFold,])
dataSet <- dataSet[-newFold,]
}
return(folds)
}
# Divides folds generated by split_into_folds into a training and validation set
CVtrainAndValidationSet <- function(folds, splitIndex = -1) {
if (splitIndex == -1 || splitIndex > length(folds)) {
testIndex = floor(runif(1, min = 1, max = length(folds)))
test = folds[[testIndex]]
train = folds
train[[testIndex]] = NULL
} else {
test = folds[[splitIndex]]
train = folds
train[[splitIndex]] = NULL
}
trainSet = train[[1]]
for (i in 2:length(train)) {
trainSet = rbind(trainSet, train[[i]])
}
return(list(trainSet,test))
}
data(spam)
folds = crossValidationSplits(spam, k = 2, seed = 12345)
Cs = c(1,5)
widths = c(0.01,0.05)
results = matrix(NA, ncol=3, nrow = 6)
colnames(results) = c("C", "Width", "Error")
i = 1
for (C in Cs) {
for(width in widths) {
err = as.numeric()
for(j in 1:2) {
model = ksvm(type ~., data = folds[[j]], C = C, cross = 2, kernel = "rbfdot", kpar=list(sigma=width))
test = folds[[i %% 2 + 1]]
#print("here")
#print(test)
table(test$type, predict(model, test))
#print("here too")
err[j] = 1 - sum(diag(table(test$type, predict(model, test))))/sum(table(test$type, predict(model, test)))
#print(" here as well")
}
results[i,] = c(C, width, mean(err))
i = i + 1
}
}
View(results)
for (C in Cs) {
err = as.numeric()
for(j in 1:2) {
model = ksvm(type ~., data = folds[[i]], C = C, cross = 2, kernel = "vanilladot")
test = folds[[i %% 2 + 1]]
table(test$type, predict(model, test))
err[j] = 1 - sum(diag(table(test$type, predict(model, test))))/sum(table(test$type, predict(model, test)))
}
results[i,] = c(C, -1, mean(err))
i = i +1
print(mean(err))
}
for (C in Cs) {
err = as.numeric()
for(j in 1:2) {
model = ksvm(type ~., data = folds[[j]], C = C, cross = 2, kernel = "vanilladot")
test = folds[[j %% 2 + 1]]
table(test$type, predict(model, test))
err[j] = 1 - sum(diag(table(test$type, predict(model, test))))/sum(table(test$type, predict(model, test)))
}
results[i,] = c(C, -1, mean(err))
i = i +1
print(mean(err))
}
View(results)
