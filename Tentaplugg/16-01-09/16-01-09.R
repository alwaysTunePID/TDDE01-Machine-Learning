###### ASSIGNMENT 1 ######

data = read.csv("crx.csv")
data$Class = as.factor(data$Class)
# Task 1 

n=dim(data)[1]
set.seed(12345)
id=sample(1:n, floor(n*0.8))
train=data[id,]
test=data[-id,]

library(tree)

tree_model = tree(Class~., data = train)

par(mfrow=c(2,1))
plot(tree_model)
text(tree_model, pretty=0)

# Remove second observation
train_new = train[-2,]
tree_model_new = tree(Class~., data = train_new)
plot(tree_model_new)
text(tree_model_new, pretty=0)

# Removing only one observation leads us to completely new results which is weird. 
# This is probably an outlier of some sorts, which makes the model adjust when it is removed. 

# Task 2 

cv_tree = cv.tree(tree_model, K = 10)
plot(cv_tree$size, cv_tree$dev, type = "o")

best_depth = cv_tree$size[which.min(cv_tree$dev)]

pruned_tree = prune.tree(tree_model, best=best_depth)

# The optimal tree has a depth of 6. 
# The variables selected 

plot(pruned_tree)
text(pruned_tree, pretty = 0)

# Task 3 

library(glmnet)

x_train = model.matrix(~ .-1, train[,-16])
y_train = train[,16]
set.seed(12345)
lasso = glmnet(x=x_train, y=y_train, family="binomial", alpha = 1)
plot(lasso, xvar="lambda", label=TRUE, main ="Cross-Validation LASSO")
lasso_cv = cv.glmnet(x=x_train, y=y_train, family = "binomial", alpha = 1)
plot(lasso_cv, xvar="lambda", label = TRUE)

lambda_min = lasso_cv$lambda.min


# Task 4

E = function(y, p) {
  return(sum(y*log(p) + (1-y)*log(1-p)))
}

x_test = model.matrix(~ .-1, test[,-16])
y_test = test[,16]
preds_lasso = predict(lasso_cv, x_test, s = "lambda.min", type = "response")
preds_tree = predict(pruned_tree, test, type = "vector")[,1]

E_lasso = E(as.numeric(test$Class), preds_lasso)
E_tree = E(as.numeric(test$Class), preds_tree)

# Lasso outperforms! This is because it is more robust and have better probability values!!!!!


###### ASSIGNMENT 2 ######

library(mboost)
bf = read.csv2("bodyfatregression.csv")
set.seed(1234567890)
m = blackboost(Bodyfat_percent ~ Waist_cm + Weight_kg, data = bf)
mstop(m)
cvf = cv(model.weights(m))
cvm = cvrisk(m, folds=cvf, grid=1:100)
plot(cvm)
mstop(cvm)

# Gradient boosting is a machine learning technique for regression and classification problems
# which produces a prediction model in the form of an ensemble of weak prediction models, 
# typically decision trees. It builds the model in a stage-wise fashion like other boosting 
# methods do, and it generalizes them by allowing optimization of an arbitrary differentiable loss function. 

# Optimal number of iterations, where the combined (mean) squared error of the model was the least
# Meaning: 35 iterations of boosting yields the best result, "proven" using cross validation on 10 different sets of 
# observations (the ones with weight 1 were included and the ones with 0 were not)
# The black line represents the mean. 

# Task 2 

library(kernlab)

# Splits the data into K folds. Default set to 5. 
# X is a matrix containing features
# Y is a vector (or Nx1 matrix) containing response variables
crossValidationSplits <- function(dataSet, k, seed = 123) {
  smp_size = floor(nrow(dataSet)/k)
  
  ## set the seed to make your partition reproducible
  set.seed(seed)
  
  folds <- list()
  
  for (i in 1:k) {
    newFold <- sample(seq_len(nrow(dataSet)), size = smp_size)
    folds[[i]] = data.frame(dataSet[newFold,])
    dataSet <- dataSet[-newFold,]
  }
  return(folds)
  
}

# Divides folds generated by split_into_folds into a training and validation set
CVtrainAndValidationSet <- function(folds, splitIndex = -1) {
  if (splitIndex == -1 || splitIndex > length(folds)) {
    testIndex = floor(runif(1, min = 1, max = length(folds)))
    test = folds[[testIndex]]
    train = folds
    train[[testIndex]] = NULL
  } else {
    test = folds[[splitIndex]]
    train = folds
    train[[splitIndex]] = NULL
  }
  trainSet = train[[1]]
  for (i in 2:length(train)) {
    trainSet = rbind(trainSet, train[[i]])
  }
  
  return(list(trainSet,test))
}

data(spam)

folds = crossValidationSplits(spam, k = 2, seed = 12345)

Cs = c(1,5)

widths = c(0.01,0.05)

# RBF kernel models

results = matrix(NA, ncol=3, nrow = 6)
colnames(results) = c("C", "Width", "Error")
i = 1
for (C in Cs) {
  for(width in widths) {
    err = as.numeric()
    for(j in 1:2) {
      model = ksvm(type ~., data = folds[[j]], C = C, cross = 2, kernel = "rbfdot", kpar=list(sigma=width))
      test = folds[[i %% 2 + 1]]
      #print("here")
      #print(test)
      table(test$type, predict(model, test))
      #print("here too")
      err[j] = 1 - sum(diag(table(test$type, predict(model, test))))/sum(table(test$type, predict(model, test)))
      #print(" here as well")
    }
    results[i,] = c(C, width, mean(err))
    i = i + 1
  }
}

# Linear kernel

for (C in Cs) {
  err = as.numeric()
  for(j in 1:2) {
    model = ksvm(type ~., data = folds[[j]], C = C, cross = 2, kernel = "vanilladot")
    test = folds[[j %% 2 + 1]]
    table(test$type, predict(model, test))
    err[j] = 1 - sum(diag(table(test$type, predict(model, test))))/sum(table(test$type, predict(model, test)))
  }
  results[i,] = c(C, -1, mean(err))
  i = i +1
  print(mean(err))
}

# Best is rbf kernel with width 0.05 and C = 1


##### ASSIGNMENT 2  - NEURAL NETWORK #####



