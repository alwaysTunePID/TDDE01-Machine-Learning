MSE_train_two_layers = as.numeric()
library(neuralnet)
MSE = function(y, y_hat) {
n = length(y)
return((1/n)*sum((y - y_hat)^2))
}
i = 1
for (thr in thresholds) {
nn_one_layer <- neuralnet(y ~ x, data = train,
threshold = thr, startweights = winit, hidden = 10)
nn_two_layers = neuralnet(y ~ x, data = train, threshold = thr, startweights = winit, hidden = c(3,3))
# Use validation set here to get error
yValidation_one_layer = compute(nn_one_layer, val$x)$net.result
yValidation_two_layers = compute(nn_two_layers, val$x)$net.result
MSE_validation_one_layer[i] = MSE(y = val$y, y_hat = yValidation_one_layer[,1])
MSE_validation_two_layers[i] = MSE(y = val$y, y_hat = yValidation_two_layers[,1])
yTrain_one_layer = compute(nn_one_layer, train$x)$net.result
yTrain_two_layers = compute(nn_two_layers, train$x)$net.result
MSE_train_one_layer[i] = MSE(y = train$y, y_hat = yTrain_one_layer[,1])
MSE_train_two_layers[i] = MSE(y = train$y, y_hat = yTrain_two_layers[,1])
i = i + 1
}
plot(nn_two_layers)
plot(x=seq(1,10,1), y = MSE_validation_one_layer, type = "l", lwd = 3, col = "red", main = "MSE for the two models", sub = "Thick lines = validation. Thinner lines = train. Red = one layer, blue = two layers")
lines(x= seq(1,10,1), y = MSE_train_one_layer, col = "red", lwd = 2)
lines(x = seq(1,10,1), y = MSE_validation_two_layers, col = "blue", lwd = 3)
lines(x = seq(1,10,1), y = MSE_train_two_layers, col = "blue", lwd = 2)
plot(x=seq(1,10,1), y = MSE_validation_one_layer, type = "l", lwd = 3, col = "red", main = "MSE for the two models", sub = "Thick lines = validation. Thinner lines = train. Red = one layer, blue = two layers", ylim = c(0,max(MSE_validation_one_layer, MSE_validation_two_layers, MSE_train_one_layer, MSE_train_two_layers)))
lines(x= seq(1,10,1), y = MSE_train_one_layer, col = "red", lwd = 2)
lines(x = seq(1,10,1), y = MSE_validation_two_layers, col = "blue", lwd = 3)
lines(x = seq(1,10,1), y = MSE_train_two_layers, col = "blue", lwd = 2)
plot(nn_two_layers)
plot(x=seq(1,10,1), y = MSE_validation_one_layer, type = "l", lwd = 3, col = "red", main = "MSE for the two models", sub = "Thick lines = validation. Thinner lines = train. Red = one layer, blue = two layers", ylim = c(0,max(MSE_validation_one_layer, MSE_validation_two_layers, MSE_train_one_layer, MSE_train_two_layers)))
lines(x= seq(1,10,1), y = MSE_train_one_layer, col = "red", lwd = 2)
lines(x = seq(1,10,1), y = MSE_validation_two_layers, col = "blue", lwd = 3)
lines(x = seq(1,10,1), y = MSE_train_two_layers, col = "blue", lwd = 2)
yTrain_one_layer[,1]
yTrain_one_layer
train$y
MSE(y = train$y, y_hat = yTrain_one_layer[,1])
plot(x=seq(1,10,1), y = MSE_validation_one_layer, type = "l", lwd = 3, col = "red", main = "MSE for the two models", sub = "Thick lines = validation. Thinner lines = train. Red = one layer, blue = two layers", ylim = c(0,max(MSE_validation_one_layer, MSE_validation_two_layers, MSE_train_one_layer, MSE_train_two_layers)))
lines(x= seq(1,10,1), y = MSE_train_one_layer, col = "red", lwd = 2)
lines(x = seq(1,10,1), y = MSE_validation_two_layers, col = "blue", lwd = 3)
lines(x = seq(1,10,1), y = MSE_train_two_layers, col = "blue", lwd = 2)
set.seed(12345)
x = runif(50, 0,10)
y = sin(x)
data = data.frame(x,y)
plot(x,y)
n=dim(data)[1]
set.seed(12345)
id=sample(1:n, floor(n*0.5))
train=data[id,]
val=data[-id,]
thresholds = seq(1,10,1)/1000
set.seed(12345)
winit = runif(50, -1,1)
MSE_validation_one_layer = as.numeric()
MSE_validation_two_layers = as.numeric()
MSE_train_one_layer = as.numeric()
MSE_train_two_layers = as.numeric()
library(neuralnet)
MSE = function(y, y_hat) {
n = length(y)
return((1/n)*sum((y - y_hat)^2))
}
i = 1
for (thr in thresholds) {
nn_one_layer <- neuralnet(y ~ x, data = train,
threshold = thr, startweights = winit, hidden = 10)
nn_two_layers = neuralnet(y ~ x, data = train,
threshold = thr, startweights = winit, hidden = c(3,3))
# Use validation set here to get error
yValidation_one_layer = compute(nn_one_layer, val$x)$net.result
yValidation_two_layers = compute(nn_two_layers, val$x)$net.result
MSE_validation_one_layer[i] = MSE(y = val$y, y_hat = yValidation_one_layer[,1])
MSE_validation_two_layers[i] = MSE(y = val$y, y_hat = yValidation_two_layers[,1])
yTrain_one_layer = compute(nn_one_layer, train$x)$net.result
yTrain_two_layers = compute(nn_two_layers, train$x)$net.result
MSE_train_one_layer[i] = MSE(y = train$y, y_hat = yTrain_one_layer[,1])
MSE_train_two_layers[i] = MSE(y = train$y, y_hat = yTrain_two_layers[,1])
i = i + 1
}
plot(nn_two_layers)
plot(x=seq(1,10,1), y = MSE_validation_one_layer, type = "l", lwd = 3, col = "red", main = "MSE for the two models", sub = "Thick lines = validation. Thinner lines = train. Red = one layer, blue = two layers", ylim = c(0,max(MSE_validation_one_layer, MSE_validation_two_layers, MSE_train_one_layer, MSE_train_two_layers)))
lines(x= seq(1,10,1), y = MSE_train_one_layer, col = "red", lwd = 2)
lines(x = seq(1,10,1), y = MSE_validation_two_layers, col = "blue", lwd = 3)
lines(x = seq(1,10,1), y = MSE_train_two_layers, col = "blue", lwd = 2)
#### Assignment 1 ####
### Task 1 ###
data = read.csv("video.csv")
result = prcomp(data[,-c(2,19)])
lambda=result$sdev^2
explanationFactor = lambda/sum(lambda)*100
names(explanationFactor) = seq(1,length(lambda),1)
plot(explanationFactor, type = "b", xlab = "Variable", ylab = "% explanatory")
explanationFactor = sort(explanationFactor, decreasing = T)
library(pls)
task2_data = data[,-c(2)]
n=dim(data)[1]
set.seed(12345)
id=sample(1:n, floor(n*0.5))
train=data[id,]
test=data[-id,]
MSE = function(y, y_hat) {
n = length(y)
return((1/n)*sum((y - y_hat)^2))
}
MSE_train = as.numeric()
MSE_test = as.numeric()
M = (ncol(task2_data) - 1)
for (m in 1:M) {
pcr_model = mvr(utime ~., ncomp = m, scale = TRUE, data = train)
train_preds = predict(pcr_model, train, ncomp = m)
test_preds = predict(pcr_model, test, ncomp = m)
MSE_train[m] = MSE(train$utime, train_preds)
MSE_test[m] = MSE(test$utime, test_preds)
}
plot(x=seq(1,M,1), y = MSE_train, col = "green", type = "l", lwd = 3, xlab = "M", ylab = "Mean squared error", main = "Red = test, green = train")
lines(x=seq(1,M,1), y = MSE_test, col = "red", type = "l", lwd = 3)
plot(x=seq(1,10,1), y = MSE_validation_one_layer, type = "l", lwd = 3, col = "red", main = "MSE for the two models", sub = "Thick lines = validation. Thinner lines = train. Red = one layer, blue = two layers", ylim = c(0,max(MSE_validation_one_layer, MSE_validation_two_layers, MSE_train_one_layer, MSE_train_two_layers)))
set.seed(12345)
x = runif(50, 0,10)
y = sin(x)
data = data.frame(x,y)
plot(x,y)
n=dim(data)[1]
set.seed(12345)
id=sample(1:n, floor(n*0.5))
train=data[id,]
val=data[-id,]
thresholds = seq(1,10,1)/1000
set.seed(12345)
winit = runif(100, -1,1)
MSE_validation_one_layer = as.numeric()
MSE_validation_two_layers = as.numeric()
MSE_train_one_layer = as.numeric()
MSE_train_two_layers = as.numeric()
library(neuralnet)
MSE = function(y, y_hat) {
n = length(y)
return((1/n)*sum((y - y_hat)^2))
}
i = 1
for (thr in thresholds) {
nn_one_layer <- neuralnet(y ~ x, data = train,
threshold = thr, startweights = winit, hidden = 10)
nn_two_layers = neuralnet(y ~ x, data = train,
threshold = thr, startweights = winit, hidden = c(3,3))
# Use validation set here to get error
yValidation_one_layer = compute(nn_one_layer, val$x)$net.result
yValidation_two_layers = compute(nn_two_layers, val$x)$net.result
MSE_validation_one_layer[i] = MSE(y = val$y, y_hat = yValidation_one_layer[,1])
MSE_validation_two_layers[i] = MSE(y = val$y, y_hat = yValidation_two_layers[,1])
yTrain_one_layer = compute(nn_one_layer, train$x)$net.result
yTrain_two_layers = compute(nn_two_layers, train$x)$net.result
MSE_train_one_layer[i] = MSE(y = train$y, y_hat = yTrain_one_layer[,1])
MSE_train_two_layers[i] = MSE(y = train$y, y_hat = yTrain_two_layers[,1])
i = i + 1
}
#plot(nn_two_layers)
plot(x=seq(1,10,1), y = MSE_validation_one_layer, type = "l", lwd = 3, col = "red", main = "MSE for the two models", sub = "Thick lines = validation. Thinner lines = train. Red = one layer, blue = two layers", ylim = c(0,max(MSE_validation_one_layer, MSE_validation_two_layers, MSE_train_one_layer, MSE_train_two_layers)))
lines(x= seq(1,10,1), y = MSE_train_one_layer, col = "red", lwd = 2)
lines(x = seq(1,10,1), y = MSE_validation_two_layers, col = "blue", lwd = 3)
lines(x = seq(1,10,1), y = MSE_train_two_layers, col = "blue", lwd = 2)
set.seed(12345)
x = runif(50, 0,10)
y = sin(x)
data = data.frame(x,y)
plot(x,y)
train = data[1:25,]
val = data[26:50,]
#n=dim(data)[1]
#set.seed(12345)
#id=sample(1:n, floor(n*0.5))
#train=data[id,]
#val=data[-id,]
thresholds = seq(1,10,1)/1000
set.seed(12345)
winit = runif(100, -1,1)
MSE_validation_one_layer = as.numeric()
MSE_validation_two_layers = as.numeric()
MSE_train_one_layer = as.numeric()
MSE_train_two_layers = as.numeric()
library(neuralnet)
MSE = function(y, y_hat) {
n = length(y)
return((1/n)*sum((y - y_hat)^2))
}
i = 1
for (thr in thresholds) {
nn_one_layer <- neuralnet(y ~ x, data = train,
threshold = thr, startweights = winit, hidden = 10)
nn_two_layers = neuralnet(y ~ x, data = train,
threshold = thr, startweights = winit, hidden = c(3,3))
# Use validation set here to get error
yValidation_one_layer = compute(nn_one_layer, val$x)$net.result
yValidation_two_layers = compute(nn_two_layers, val$x)$net.result
MSE_validation_one_layer[i] = MSE(y = val$y, y_hat = yValidation_one_layer[,1])
MSE_validation_two_layers[i] = MSE(y = val$y, y_hat = yValidation_two_layers[,1])
yTrain_one_layer = compute(nn_one_layer, train$x)$net.result
yTrain_two_layers = compute(nn_two_layers, train$x)$net.result
MSE_train_one_layer[i] = MSE(y = train$y, y_hat = yTrain_one_layer[,1])
MSE_train_two_layers[i] = MSE(y = train$y, y_hat = yTrain_two_layers[,1])
i = i + 1
}
#plot(nn_two_layers)
plot(x=seq(1,10,1), y = MSE_validation_one_layer, type = "l", lwd = 3, col = "red", main = "MSE for the two models", sub = "Thick lines = validation. Thinner lines = train. Red = one layer, blue = two layers", ylim = c(0,max(MSE_validation_one_layer, MSE_validation_two_layers, MSE_train_one_layer, MSE_train_two_layers)))
lines(x= seq(1,10,1), y = MSE_train_one_layer, col = "red", lwd = 2)
lines(x = seq(1,10,1), y = MSE_validation_two_layers, col = "blue", lwd = 3)
lines(x = seq(1,10,1), y = MSE_train_two_layers, col = "blue", lwd = 2)
library(neuralnet)
set.seed(1234567890)
Var <- runif(50, 0, 10)
trva <- data.frame(Var, Sin=sin(Var))
tr <- trva[1:25,] # Training
va <- trva[26:50,] # Validation
restr <- vector(length = 10)
resva <- vector(length = 10)
winit <- runif(22, -1, 1) # Random initializaiton of the weights in the interval [-1, 1]
for(i in 1:10) {
nn <- neuralnet(formula = Sin ~ Var, data = tr, hidden = c(3,3), startweights = winit,
threshold = i/1000, lifesign = "full")
# nn$result.matrix
aux <- compute(nn, tr[,1])$net.result # Compute predictions for the trainig set and their squared error
restr[i] <- sum((tr[,2] - aux)**2)/2
aux <- compute(nn, va[,1])$net.result # The same for the validation set
resva[i] <- sum((va[,2] - aux)**2)/2
}
plot(restr, type = "o")
plot(resva, type = "o")
restr
resva
set.seed(1234567890)
Var <- runif(50, 0, 10)
trva <- data.frame(Var, Sin=sin(Var))
tr <- trva[1:25,] # Training
va <- trva[26:50,] # Validation
restr <- vector(length = 10)
resva <- vector(length = 10)
winit <- runif(41, -1, 1) # Random initializaiton of the weights in the interval [-1, 1]
for(i in 1:10) {
nn <- neuralnet(formula = Sin ~ Var, data = tr, hidden = c(10), startweights = winit,
threshold = i/1000, lifesign = "full")
# nn$result.matrix
aux <- compute(nn, tr[,1])$net.result # Compute predictions for the trainig set and their squared error
restr[i] <- sum((tr[,2] - aux)**2)/2
aux <- compute(nn, va[,1])$net.result # The same for the validation set
resva[i] <- sum((va[,2] - aux)**2)/2
}
plot(restr, type = "o")
plot(resva, type = "o")
restr
resva
# JMP
library(kernlab)
set.seed(1234567890)
data(spam)
# Model selection
index <- sample(1:4601)
tr <- spam[index[1:2500], ]
va <- spam[index[2501:3501], ]
te <- spam[index[3502:4601], ]
filter <- ksvm(type~.,data=tr,kernel="rbfdot",kpar=list(sigma=0.05),C=0.5)
mailtype <- predict(filter,va[,-58])
t <- table(mailtype,va[,58])
(t[1,2]+t[2,1])/sum(t)
filter <- ksvm(type~.,data=tr,kernel="rbfdot",kpar=list(sigma=0.05),C=1)
mailtype <- predict(filter,va[,-58])
t <- table(mailtype,va[,58])
(t[1,2]+t[2,1])/sum(t)
filter <- ksvm(type~.,data=tr,kernel="rbfdot",kpar=list(sigma=0.05),C=5)
mailtype <- predict(filter,va[,-58])
t <- table(mailtype,va[,58])
(t[1,2]+t[2,1])/sum(t)
filter <- ksvm(type~.,data=spam[index[1:3501], ],kernel="rbfdot",kpar=list(sigma=0.05),C=1)
mailtype <- predict(filter,te[,-58])
t <- table(mailtype,te[,58])
(t[1,2]+t[2,1])/sum(t)
filter <- ksvm(type~.,data=spam,kernel="rbfdot",kpar=list(sigma=0.05),C=1)
#### Assignment 1 ####
### Task 1 ###
data = read.csv("video.csv")
result = prcomp(data[,-c(2,19)])
lambda=result$sdev^2
explanationFactor = lambda/sum(lambda)*100
names(explanationFactor) = seq(1,length(lambda),1)
plot(explanationFactor, type = "b", xlab = "Variable", ylab = "% explanatory")
explanationFactor = sort(explanationFactor, decreasing = T)
# Only 1 variable is needed when scaling is not done.
# This is because.
# Task 2
library(pls)
task2_data = data[,-c(2)]
n=dim(data)[1]
set.seed(12345)
id=sample(1:n, floor(n*0.5))
train=data[id,]
test=data[-id,]
MSE = function(y, y_hat) {
n = length(y)
return((1/n)*sum((y - y_hat)^2))
}
MSE_train = as.numeric()
MSE_test = as.numeric()
M = (ncol(task2_data) - 1)
for (m in 1:M) {
pcr_model = mvr(utime ~., ncomp = m, scale = TRUE, data = train)
train_preds = predict(pcr_model, train, ncomp = m)
test_preds = predict(pcr_model, test, ncomp = m)
MSE_train[m] = MSE(train$utime, train_preds)
MSE_test[m] = MSE(test$utime, test_preds)
}
plot(x=seq(1,M,1), y = MSE_train, col = "green", type = "l", lwd = 3, xlab = "M", ylab = "Mean squared error", main = "Red = test, green = train")
lines(x=seq(1,M,1), y = MSE_test, col = "red", type = "l", lwd = 3)
data0=read.csv("video.csv")
data1=data0
data1$codec=c()
n=dim(data1)[1]
set.seed(12345)
id=sample(1:n, floor(n*0.5))
train=data1[id,]
test=data1[-id,]
data11=data1
data11$utime=c()
res=prcomp(data11)
lambda=res$sdev^2
sprintf("%2.3f",cumsum(lambda)/sum(lambda)*100)
res=prcomp(scale(data11))
lambda=res$sdev^2
sprintf("%2.3f",cumsum(lambda)/sum(lambda)*100)
library(pls)
trE=numeric(17)
testE=numeric(17)
for (i in 1:17){
pcrN=pcr(utime~., 17, data=train,  scale=T)
Yf=predict(pcrN, ncomp=i)
Yt=predict(pcrN, newdata=test, ncomp=i)
trE[i]=mean((train$utime-Yf)^2)
testE[i]=mean((test$utime-Yt)^2)
}
plot(testE, type="l", col="red", ylim=c(100,300), ylab="Error")
points(trE, type="l", col="blue")
#### Assignment 1 ####
### Task 1 ###
data = read.csv("video.csv")
result = prcomp(data[,-c(2,19)])
lambda=result$sdev^2
explanationFactor = lambda/sum(lambda)*100
names(explanationFactor) = seq(1,length(lambda),1)
plot(explanationFactor, type = "b", xlab = "Variable", ylab = "% explanatory")
explanationFactor = sort(explanationFactor, decreasing = T)
# Only 1 variable is needed when scaling is not done.
# This is because.
# Task 2
library(pls)
task2_data = data[,-c(2)]
n=dim(data)[1]
set.seed(12345)
id=sample(1:n, floor(n*0.5))
train=data[id,]
test=data[-id,]
MSE = function(y, y_hat) {
n = length(y)
#return((1/n)*sum((y - y_hat)^2))
return(mean((y - y_hat)^2))
}
MSE_train = as.numeric()
MSE_test = as.numeric()
M = (ncol(task2_data) - 1)
for (m in 1:M) {
pcr_model = mvr(utime ~., ncomp = m, scale = TRUE, data = train)
train_preds = predict(pcr_model, train, ncomp = m)
test_preds = predict(pcr_model, test, ncomp = m)
MSE_train[m] = MSE(train$utime, train_preds)
MSE_test[m] = MSE(test$utime, test_preds)
}
plot(x=seq(1,M,1), y = MSE_train, col = "green", type = "l", lwd = 3, xlab = "M", ylab = "Mean squared error", main = "Red = test, green = train")
lines(x=seq(1,M,1), y = MSE_test, col = "red", type = "l", lwd = 3)
#### Assignment 1 ####
### Task 1 ###
data = read.csv("video.csv")
result = prcomp(data[,-c(2,19)])
lambda=result$sdev^2
explanationFactor = lambda/sum(lambda)*100
names(explanationFactor) = seq(1,length(lambda),1)
plot(explanationFactor, type = "b", xlab = "Variable", ylab = "% explanatory")
explanationFactor = sort(explanationFactor, decreasing = T)
# Only 1 variable is needed when scaling is not done.
# This is because.
# Task 2
library(pls)
task2_data = data[,-c(2)]
n=dim(data)[1]
set.seed(12345)
id=sample(1:n, floor(n*0.5))
train=data[id,]
test=data[-id,]
MSE = function(y, y_hat) {
n = length(y)
#return((1/n)*sum((y - y_hat)^2))
return(mean((y - y_hat)^2))
}
MSE_train = as.numeric()
MSE_test = as.numeric()
M = (ncol(task2_data) - 1)
for (m in 1:M) {
pcr_model = mvr(utime ~., scale = TRUE, data = train)
train_preds = predict(pcr_model, train, ncomp = m)
test_preds = predict(pcr_model, test, ncomp = m)
MSE_train[m] = MSE(train$utime, train_preds)
MSE_test[m] = MSE(test$utime, test_preds)
}
plot(x=seq(1,M,1), y = MSE_train, col = "green", type = "l", lwd = 3, xlab = "M", ylab = "Mean squared error", main = "Red = test, green = train")
lines(x=seq(1,M,1), y = MSE_test, col = "red", type = "l", lwd = 3)
testE
MSE_test
m = 8
pcr_model = mvr(utime ~., ncomp = m, scale = TRUE, data = train)
summary(pcr_model)
pcr_model$coefficients
Yloadings(pcr_model)
#### Assignment 1 ####
### Task 1 ###
data = read.csv("video.csv")
result = prcomp(data[,-c(2,19)])
lambda=result$sdev^2
explanationFactor = lambda/sum(lambda)*100
names(explanationFactor) = seq(1,length(lambda),1)
plot(explanationFactor, type = "b", xlab = "Variable", ylab = "% explanatory")
explanationFactor = sort(explanationFactor, decreasing = T)
# Only 1 variable is needed when scaling is not done.
# This is because.
# Task 2
library(pls)
task2_data = data[,-c(2)]
n=dim(data)[1]
set.seed(12345)
id=sample(1:n, floor(n*0.5))
train=data[id,]
test=data[-id,]
MSE = function(y, y_hat) {
n = length(y)
#return((1/n)*sum((y - y_hat)^2))
return(mean((y - y_hat)^2))
}
MSE_train = as.numeric()
MSE_test = as.numeric()
M = (ncol(task2_data) - 1)
for (m in 1:M) {
pcr_model = mvr(utime ~., scale = TRUE, data = train, validation = "none")
train_preds = predict(pcr_model, train, ncomp = m)
test_preds = predict(pcr_model, test, ncomp = m)
MSE_train[m] = MSE(train$utime, train_preds)
MSE_test[m] = MSE(test$utime, test_preds)
}
plot(x=seq(1,M,1), y = MSE_train, col = "green", type = "l", lwd = 3, xlab = "M", ylab = "Mean squared error", main = "Red = test, green = train")
lines(x=seq(1,M,1), y = MSE_test, col = "red", type = "l", lwd = 3)
m = 8
pcr_model = mvr(utime ~., ncomp = m, scale = TRUE, data = train, validation = "none")
summary(pcr_model)
pcr_model$coefficients
Yloadings(pcr_model)
data$class = factor(apply(as.matrix(data$codec), 1, function(val) {
if (val == "mpeg4") {
return("mpeg")
} else {
return("other")
}
}))
mpegs = data[data$class == "mpeg",]
others = data[data$class == "other",]
plot(mpegs$duration, mpegs$frames, col = "red", xlim = c(min(data$duration), max(data$duration)), ylim = c(min(data$frames),max(data$frames)), main = "Blue = others, red = mpeg")
points(others$duration, others$frames, col = "blue")
print("Yes, it does look linearly separable")
library(MASS)
task_5_data = data[, c(1,10,20)]
task_5_data$duration = scale(task_5_data$duration)
task_5_data$frames = scale(task_5_data$frames)
plot(task_5_data[task_5_data$class == "mpeg",]$duration, task_5_data[task_5_data$class == "mpeg",]$frames, col = "red")
points(task_5_data[task_5_data$class == "other",]$duration, task_5_data[task_5_data$class == "other",]$frames, col = "blue")
lda_model = lda(class ~., data = task_5_data)
summary(lda_model)
preds = predict(lda_model, task_5_data)
conf_m = table(Actual = task_5_data$class, Predicted = preds$class)
print(conf_m)
print(1 - sum(diag(conf_m))/sum(conf_m))
cov(task_5_data$duration, task_5_data$frames)
cov(task_5_data)
cov(task_5_data$duration, task_5_data$frames)
cov(seq(1,10,1), seq(1,10,1))
cor(task_5_data$duration, task_5_data$frames)
task_6_data = data[, c(1,10,20)]
library(tree)
tree_model = tree(class ~.,task_6_data)
cv = cv.tree(tree_model)
optimal_size = cv$size[which.min(cv$dev)]
finalTree=prune.tree(tree_model, best=optimal_size)
plot(finalTree)
text(finalTree, pretty = 0)
# Training error
preds = factor(apply(predict(finalTree, task_6_data), 1, function(row) {
best = which.max(row)
if (best == 1) {
return("mpeg")
} else {
return("other")
}
}))
conf_m = table(Actual = task_6_data$class, Predictions = preds)
print(conf_m)
print("There are many leaves since there are many smaller special cases and a more dense area in one place. ")
